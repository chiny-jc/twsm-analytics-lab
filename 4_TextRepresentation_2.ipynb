{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"4_TextRepresentation_2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"92e90cb2be5a4a93bf71fe9cdf6a0f35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fac6b6fbe67249e29b758d9c88cd3387","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b079faa8de0047a794934ddf3f7884b4","IPY_MODEL_ff4ff8f34d7c4ce6998b8a63359df5a9"]}},"fac6b6fbe67249e29b758d9c88cd3387":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b079faa8de0047a794934ddf3f7884b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_eb8413b73c0e43a0b7d74e232473cdcd","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fffabd88a31f4d65b506393ff608f5be"}},"ff4ff8f34d7c4ce6998b8a63359df5a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_96980f31973145728c11ad45160b5534","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:02&lt;00:00, 96.6kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_27b922c27c854b0d9a16fc49f39e9eb1"}},"eb8413b73c0e43a0b7d74e232473cdcd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fffabd88a31f4d65b506393ff608f5be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"96980f31973145728c11ad45160b5534":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"27b922c27c854b0d9a16fc49f39e9eb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"612de26177af4aadb2e58b8e433a45aa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ecaaa56039a14907aafbde0435841ebe","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_70f82cda582047e6a2017ae7a0c90be6","IPY_MODEL_af274c7d57b3425c93eb79c9f587e8b6"]}},"ecaaa56039a14907aafbde0435841ebe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70f82cda582047e6a2017ae7a0c90be6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5e9834da11f6424bb811f6827032ccd7","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7c2baed9200d43ae88847954079656b6"}},"af274c7d57b3425c93eb79c9f587e8b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_84beac45c3724120b639841d81d2cd82","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:01&lt;00:00, 27.3B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5f7bbf0b9d744126b844d0b2d101fc73"}},"5e9834da11f6424bb811f6827032ccd7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7c2baed9200d43ae88847954079656b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"84beac45c3724120b639841d81d2cd82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5f7bbf0b9d744126b844d0b2d101fc73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b57f7a3335a44b1db49ed487c7ae0478":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ef6bb1ae04c342a299465f4663dcc465","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3b74d7788c4f4ce6a497364c98fdf462","IPY_MODEL_58d16f70ca0f4b84a8d9cda08f63946b"]}},"ef6bb1ae04c342a299465f4663dcc465":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b74d7788c4f4ce6a497364c98fdf462":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_69f33c5793f14fa193c2156a2085c5b3","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1c8b0938e65740e3946b8a05fe9a3cdf"}},"58d16f70ca0f4b84a8d9cda08f63946b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cefc4df08efb4a2188b02dccaed2eea5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 1.04MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_458f248e91514f3f9313f5f12d9f012f"}},"69f33c5793f14fa193c2156a2085c5b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1c8b0938e65740e3946b8a05fe9a3cdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cefc4df08efb4a2188b02dccaed2eea5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"458f248e91514f3f9313f5f12d9f012f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f725096a2ae41d8abe96106bc8a2af8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b6f26d88ecec437094c20a40e8ad61ad","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bcd5368ee1064281a91365c04ed91a12","IPY_MODEL_7f94638b6e124e32a51a3c349a417718"]}},"b6f26d88ecec437094c20a40e8ad61ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bcd5368ee1064281a91365c04ed91a12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e00e4af94e5246c7825528826c63858d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6e31be56a7b6420abbfc8cc760f7da32"}},"7f94638b6e124e32a51a3c349a417718":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b94e7b1063d9458f8c58256dfaa8ff27","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 1.79kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_48cad35fca264b4bb73380919f9107fb"}},"e00e4af94e5246c7825528826c63858d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6e31be56a7b6420abbfc8cc760f7da32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b94e7b1063d9458f8c58256dfaa8ff27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"48cad35fca264b4bb73380919f9107fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ba0b2d2543994ceb87ebd8249d0d26d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c42f1d509d454550b577dc61b00dafd2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e8dcd3eac8da4e5e941025ae06a549be","IPY_MODEL_a74d5bbddd3c4223a0fa3ef2a152f48c"]}},"c42f1d509d454550b577dc61b00dafd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8dcd3eac8da4e5e941025ae06a549be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8120dd3346864259bbf360f09988e1c1","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e18db08afcc4526a05b62785840c8b4"}},"a74d5bbddd3c4223a0fa3ef2a152f48c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_411cc332b39540f5b3c518c3b5ca7b83","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:26&lt;00:00, 16.5MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c4e917dabd954958b33096f5543920f9"}},"8120dd3346864259bbf360f09988e1c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0e18db08afcc4526a05b62785840c8b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"411cc332b39540f5b3c518c3b5ca7b83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c4e917dabd954958b33096f5543920f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"X1B_WNUCYOo3"},"source":["# Exercise 4. Text Representation (2)\n","### Text, Web and Social Media Analytics"]},{"cell_type":"markdown","metadata":{"id":"8fXzDsVMYu3Q"},"source":["In this exercise, we will apply the following models to the data from the previous exercise: \n","\n","- Word2Vec\n","- Doc2Vec\n","- BERT (lemmatized)\n","\n","At the end of the exercise, we will derive a corpus with each of them which can be used in later tasks such as classification and clustering. "]},{"cell_type":"markdown","metadata":{"id":"YQRuNC5Kenv-"},"source":["## Part 0. Preparation\n","\n","As a first step, we first install the transformers package, which includes state-of-the-art NLP models for TensorFlow and Pytorch."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ajxKWnokYEwX","executionInfo":{"status":"ok","timestamp":1620655531695,"user_tz":-120,"elapsed":7744,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"b0c11e44-74a6-4981-ba24-1fa3359a197b"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 10.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 37.8MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 38.3MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LSanOt1tfayl"},"source":["We now import all the necessary packages and libraries that we will be using."]},{"cell_type":"code","metadata":{"id":"GTlhsKBbZSm5"},"source":["import pickle\n","import numpy as np\n","import pandas as pd\n","from gensim.models import Word2Vec\n","from scipy.spatial.distance import cosine\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","import tensorflow as tf\n","import torch\n","from transformers import BertTokenizer, BertModel\n","from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0XLNX4Pnfyfx"},"source":["We load our lemmatized data from the pickle file that we have saved in one of the previous exercises. We also print the first row to see that the data was loaded correctly."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsUmWDUbZyz9","executionInfo":{"status":"ok","timestamp":1620655539428,"user_tz":-120,"elapsed":15464,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"9fc3afdd-89e4-4538-a016-9c46ee2d7ace"},"source":["lemmatized_data = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/TWSM Analytics Lab/storage/lemmatized_data.p', 'rb'))\n","\n","print(lemmatized_data.iloc[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["content         car wonder enlighten car see day door sport ca...\n","target                                                          7\n","target_names                                            rec.autos\n","Name: 0, dtype: object\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"73A0GSwHg0AQ"},"source":["After loading the data, we take each document and tokenize it, so we end up with a list of all the words in the document. We print the first record to see how the data looks like."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kUfLH4Lbaku8","executionInfo":{"status":"ok","timestamp":1620655539429,"user_tz":-120,"elapsed":15461,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"10abb472-ae06-4a3f-902c-743f073a97a9"},"source":["corpus_gen = lemmatized_data['content'].apply(lambda text: text.split())\n","\n","print(corpus_gen[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['car', 'wonder', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'small', 'addition', 'bumper', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'engine', 'specs', 'year', 'production', 'car', 'history', 'info', 'funky', 'looking', 'car', 'mail', 'thank']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IoJg0w3AhL3c"},"source":["## Part 1. Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"b0UgcX_GYRhe"},"source":["Here we define a new Word2Vec model and train it using the corpus we created in the previous step. We set the 'size' parameter to 100 to define the dimensionality of the word vectors; we also set the 'min_count' parameter, which indicates to ignore all words that appear in less than 566 documents. After that, we save the model."]},{"cell_type":"code","metadata":{"id":"dkdpCEJRa0co"},"source":["model = Word2Vec(corpus_gen, size=100, min_count=566)\n","model.save('word2vec.model')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VSbT8Q8HZI0w"},"source":["In order to get a better idea about certain characteristics of the model, we print the features and the number of features.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-sQ3Ky4lhgIS","executionInfo":{"status":"ok","timestamp":1620658652521,"user_tz":-120,"elapsed":4607,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"8dc88a58-32ce-4165-d369-37cd413084be"},"source":["print('Features:\\n{}\\n'.format(sorted(model.wv.vocab.keys())))\n","print('Num. of Features:\\n{}'.format(len(model.wv.vocab.keys())))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Features:\n","['able', 'accept', 'access', 'act', 'action', 'actually', 'add', 'address', 'advance', 'ago', 'agree', 'allow', 'american', 'answer', 'anybody', 'appear', 'apple', 'application', 'apply', 'appreciate', 'apr', 'april', 'area', 'argument', 'armenian', 'armenians', 'article', 'ask', 'assume', 'atheist', 'attack', 'available', 'away', 'bad', 'base', 'begin', 'believe', 'better', 'bible', 'big', 'bike', 'bit', 'black', 'board', 'body', 'book', 'box', 'break', 'bring', 'build', 'buy', 'call', 'car', 'card', 'care', 'carry', 'case', 'cause', 'center', 'certain', 'certainly', 'change', 'check', 'child', 'chip', 'christian', 'christians', 'church', 'city', 'claim', 'clear', 'clinton', 'clipper', 'close', 'code', 'color', 'com', 'come', 'comment', 'company', 'consider', 'contact', 'contain', 'continue', 'control', 'copy', 'correct', 'cost', 'country', 'couple', 'course', 'cover', 'create', 'crime', 'current', 'data', 'date', 'datum', 'david', 'day', 'deal', 'death', 'decide', 'design', 'device', 'die', 'difference', 'different', 'discussion', 'disk', 'display', 'dos', 'drive', 'driver', 'drug', 'early', 'earth', 'easy', 'edu', 'effect', 'email', 'encryption', 'end', 'entry', 'error', 'event', 'evidence', 'example', 'exist', 'expect', 'experience', 'explain', 'face', 'fact', 'faith', 'faq', 'far', 'fast', 'fax', 'feel', 'file', 'follow', 'force', 'form', 'format', 'free', 'friend', 'ftp', 'function', 'game', 'general', 'get', 'give', 'go', 'god', 'good', 'gov', 'government', 'graphic', 'great', 'ground', 'group', 'guess', 'gun', 'guy', 'hand', 'happen', 'hard', 'hardware', 'have', 'head', 'hear', 'hell', 'help', 'high', 'history', 'hit', 'hockey', 'hold', 'home', 'hope', 'house', 'human', 'ibm', 'idea', 'image', 'important', 'include', 'individual', 'info', 'information', 'instead', 'interested', 'internet', 'involve', 'isn', 'israel', 'israeli', 'issue', 'jesus', 'jewish', 'jews', 'job', 'john', 'key', 'kill', 'kind', 'know', 'large', 'later', 'law', 'lead', 'leave', 'let', 'level', 'life', 'light', 'like', 'likely', 'line', 'list', 'little', 'live', 'local', 'long', 'look', 'lose', 'lot', 'love', 'low', 'mac', 'machine', 'mail', 'major', 'make', 'man', 'mark', 'matter', 'max', 'maybe', 'mean', 'member', 'memory', 'mention', 'message', 'michael', 'mike', 'mind', 'mit', 'mode', 'model', 'money', 'monitor', 'month', 'nasa', 'national', 'need', 'net', 'netcom', 'network', 'new', 'news', 'non', 'note', 'number', 'offer', 'old', 'open', 'opinion', 'order', 'org', 'original', 'output', 'package', 'pass', 'paul', 'pay', 'people', 'period', 'person', 'phone', 'place', 'plan', 'play', 'player', 'point', 'position', 'possible', 'post', 'power', 'present', 'president', 'press', 'pretty', 'price', 'probably', 'problem', 'product', 'program', 'protect', 'provide', 'pub', 'public', 'purpose', 'question', 'quote', 'rate', 'read', 'real', 'reason', 'receive', 'reference', 'release', 'religion', 'remember', 'reply', 'report', 'request', 'require', 'research', 'result', 'return', 'right', 'rule', 'run', 'sale', 'save', 'say', 'science', 'screen', 'scsi', 'season', 'second', 'security', 'see', 'sell', 'send', 'sense', 'server', 'service', 'set', 'short', 'show', 'similar', 'simple', 'simply', 'single', 'site', 'situation', 'size', 'small', 'software', 'sort', 'sound', 'source', 'space', 'speak', 'speed', 'stand', 'standard', 'start', 'state', 'statement', 'steve', 'stop', 'study', 'stuff', 'subject', 'suggest', 'sun', 'support', 'sure', 'system', 'take', 'talk', 'team', 'technology', 'tell', 'term', 'test', 'text', 'thank', 'thing', 'think', 'time', 'today', 'true', 'truth', 'try', 'turkish', 'turn', 'type', 'understand', 'university', 'use', 'user', 'uucp', 'value', 'version', 'video', 'view', 'want', 'war', 'washington', 'way', 'weapon', 'week', 'well', 'white', 'win', 'window', 'windows', 'woman', 'wonder', 'word', 'work', 'world', 'wouldn', 'write', 'wrong', 'year', 'yes']\n","\n","Num. of Features:\n","422\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fZH-G8hYZlZ3"},"source":["Here we show the vector representation of the word 'car', which has a dimension of 100 like we defined previously in the 'size' parameter when creating the model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KAotfx8nilUF","executionInfo":{"status":"ok","timestamp":1620658652522,"user_tz":-120,"elapsed":4602,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"180d6332-84fa-4365-9577-7d6239b8c299"},"source":["model.wv['car']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.016478  ,  0.47469488, -0.0735962 , -1.5947578 , -0.71565026,\n","       -0.13002563,  0.92253655, -0.56696934,  0.4373428 , -0.16656286,\n","       -1.2158672 ,  0.14024894, -0.48905385, -0.10485542, -0.293814  ,\n","       -1.8627863 ,  0.1057101 , -0.46848246, -0.10593803, -0.5915618 ,\n","       -0.79466665,  1.5016022 ,  0.9591846 , -0.9669569 , -0.7194357 ,\n","        0.5617788 ,  1.0216058 , -1.7020634 , -0.47521907, -0.37625855,\n","        0.6199494 ,  1.5019242 ,  0.42780042,  0.44220775, -0.34955272,\n","       -1.3595812 , -0.6390408 ,  1.490715  , -1.0162692 ,  0.7565516 ,\n","       -0.32053503, -0.9799328 , -0.05697813,  0.1426159 ,  1.4504987 ,\n","       -0.21290368,  0.3151061 ,  0.6335179 , -0.22196509,  0.5143563 ,\n","        0.9986693 , -0.81887585,  1.2004273 , -0.50767195,  0.44180468,\n","        0.04145134,  1.1315997 , -1.2109666 , -0.810632  , -1.1241968 ,\n","        0.96450996,  0.6558029 ,  0.94009155, -0.6905603 ,  0.890224  ,\n","        1.3099073 ,  0.78042006, -1.3137709 , -0.1449007 , -0.1787022 ,\n","       -0.92667633, -0.7575929 ,  0.13601923, -0.50129145,  0.70179933,\n","       -1.1587746 ,  2.847164  ,  0.19026104, -0.7227811 ,  0.15567152,\n","        0.44957805, -1.7161812 ,  0.91181564, -0.66540694,  1.0718646 ,\n","        1.2641098 ,  0.01383967,  1.7724216 ,  1.2576292 , -1.6462095 ,\n","       -0.58994526, -0.42224377, -0.29060718, -0.6717832 , -0.00381069,\n","        0.27556685,  0.28966582,  0.63176525, -1.4155138 , -0.30053282],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"D_CXlMWLZ0bk"},"source":["We can also check which words are similar or close in the vector space to a given one, in this case, 'car'."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-b5pssQfizuv","executionInfo":{"status":"ok","timestamp":1620658652522,"user_tz":-120,"elapsed":4594,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"e32af7c7-7e19-448d-f18c-b9a85d77f267"},"source":["model.wv.most_similar('car')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('bike', 0.6044567227363586),\n"," ('buy', 0.5782100558280945),\n"," ('friend', 0.5530164241790771),\n"," ('get', 0.49781185388565063),\n"," ('pay', 0.4936054050922394),\n"," ('price', 0.4751429557800293),\n"," ('speed', 0.4499210715293884),\n"," ('hit', 0.4419363737106323),\n"," ('drive', 0.44047486782073975),\n"," ('light', 0.4325913190841675)]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"Z1i7RnUcaz2U"},"source":["We can check the similarity to more than one word, in this case for 'bike' and 'machine', where we want to see the most similar one by defining the 'topn' to one. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJFQxhA0j56W","executionInfo":{"status":"ok","timestamp":1620658652523,"user_tz":-120,"elapsed":4589,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"4ec67198-ae2e-47c2-e51e-194d8988b2a6"},"source":["model.wv.most_similar(positive=['bike', 'machine'], topn=1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('fast', 0.654640793800354)]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"cc19UJ0gfrLe"},"source":["After seeing that each word is represented in a vector, we would like to also convert each document to a vector that is the average of all the words. To do this, we go through each word of each document and get its vector representation. After we have the vector for all the words, we average them and get a vector representation for the document. "]},{"cell_type":"code","metadata":{"id":"Eu2h4_ZrkWzO"},"source":["embedded_corpus = []\n","\n","for document in corpus_gen:\n","  word_embeddings = []\n","  for word in document:\n","    try:\n","      word_embeddings.append(model.wv[word])\n","    except KeyError:\n","      continue\n","  if len(word_embeddings) > 0:\n","    document_embedding = np.mean(word_embeddings, axis=0)\n","    embedded_corpus.append(document_embedding)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zB9AFSiLgK3c"},"source":["We then convert this document embedding to a dataframe, where each row is a document and all the columns are an average of the vector values for each word. We then print the head to understand how our dataframe looks like and we also print the shape of the dataframe to understand its structure. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IlXwsNMynUDL","executionInfo":{"status":"ok","timestamp":1620658656379,"user_tz":-120,"elapsed":8437,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"dc83c376-34a5-4649-e2b8-3ef276e384fb"},"source":["embeddings_df = pd.DataFrame(embedded_corpus)\n","\n","print(embeddings_df.head())\n","print('\\nShape: {}'.format(embeddings_df.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["         0         1         2   ...        97        98        99\n","0  0.145009  0.074307  0.062820  ... -0.002585 -0.446552 -0.051636\n","1 -0.328259 -0.023729  0.118994  ...  0.053510 -0.314329 -0.025866\n","2 -0.236099  0.099917  0.135177  ... -0.062771 -0.287006 -0.233269\n","3 -0.143364 -0.172010  0.220363  ...  0.060539 -0.210669 -0.076263\n","4 -0.043743  0.041052 -0.155883  ... -0.056644 -0.083022 -0.262626\n","\n","[5 rows x 100 columns]\n","\n","Shape: (11297, 100)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z7XHNp-Gggxo"},"source":["We now save our Word2Vec model as a pickle file so we can access it later."]},{"cell_type":"code","metadata":{"id":"m8hmhm6mtgTa"},"source":["pickle.dump(embeddings_df, open('/content/drive/MyDrive/Colab Notebooks/TWSM Analytics Lab/storage/WordtoVecModel.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VzLJtv2PguYN"},"source":["We know that the document embedding that we performed before has the same dimension as the word embedding, since it was made by averaging all the word vectors. This means we can also look for words that have a similar representation as for the document; to do this we take the first document embedding and lookup for similar words by vector. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5pIUHuoWtxRb","executionInfo":{"status":"ok","timestamp":1620658657526,"user_tz":-120,"elapsed":9576,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"de959374-7a72-4615-8a60-5ed16c0442ce"},"source":["first_document = embedded_corpus[0]\n","\n","model.wv.similar_by_vector(first_document)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('car', 0.872447669506073),\n"," ('friend', 0.6493353843688965),\n"," ('bike', 0.5698528289794922),\n"," ('get', 0.5553348064422607),\n"," ('buy', 0.5460785627365112),\n"," ('month', 0.5265671610832214),\n"," ('price', 0.513207733631134),\n"," ('see', 0.49368739128112793),\n"," ('go', 0.47965988516807556),\n"," ('lot', 0.4707129895687103)]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"dJpD7JIRhcBD"},"source":["We can also look for documents that are similar with each other by calculating the cosine distance. In order to do this, we take one document, in this case, the first one, and calculate the cosine distance with all the other documents, then we get the document who had the least distance. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WNTuXJBrwxdu","executionInfo":{"status":"ok","timestamp":1620658999447,"user_tz":-120,"elapsed":1415,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"f5214736-3dc8-47c7-e83b-ace344a185d1"},"source":["min_distance = 1.0\n","most_similar_index = 0\n","counter = 0\n","\n","for document in embedded_corpus:\n","  distance = cosine(first_document, document)\n","  if (distance < min_distance) and (distance != 0.0):\n","    min_distance = distance\n","    most_similar_index = counter\n","  counter += 1\n","\n","most_similar_index, min_distance"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1044, 0.10285025835037231)"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"uGk72YKAhzvl"},"source":["However, when we print the document that is supposed to be similar, we see that they are actually quite different. This means that the document representation that we performed by averaging all the word embeddings might not work correctly."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cVXCJ2WWx81f","executionInfo":{"status":"ok","timestamp":1620659006887,"user_tz":-120,"elapsed":669,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"adca8231-cbb5-41ab-9868-5216c56fea2f"},"source":["print(lemmatized_data['content'].iloc[0])\n","print(lemmatized_data['content'].iloc[most_similar_index])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["car wonder enlighten car see day door sport car look late early call bricklin door small addition bumper separate rest body know tellme model engine specs year production car history info funky looking car mail thank\n","old simms article apr csx cciw csx cciw stewart beal write article netnews upenn edu jhaines eniac seas upenn edu jason haines write wonder people good use old simms bunch apple mac know lot people try sell get inovative use want buy simms interested hearing guy work take use cyano acrylate glue wide panel constructs box use pencil holder get entreprenuerial spirit cheapy clear plastic box mount simm inside sell pet simm sure plenty sucker aaron\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TeNDNuog2QLQ"},"source":["## Part 2. Doc2Vec"]},{"cell_type":"markdown","metadata":{"id":"qe2Eo-rojS7M"},"source":["We now define a new Doc2Vec model and train it, where we define the 'vector_size' to 100 and the 'min_count' to 566, just like we did with the previous model. However, the different is that instead of giving the model the raw corpus, we have to create tagged documents for it to work properly."]},{"cell_type":"code","metadata":{"id":"7W9EWAUf2SzE"},"source":["documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_gen)]\n","model = Doc2Vec(documents, vector_size=100, min_count=566)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LexjfHcXj3sg"},"source":["We see that this model creates an embedding for the whole document, instead of for each word in the document. We print here the embedded first document to see how it looks like. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xz7alTYZ87WH","executionInfo":{"elapsed":38436,"status":"ok","timestamp":1620313245240,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"},"user_tz":-120},"outputId":"61cd1c00-759f-4e3a-9a65-92ccfe9a8e43"},"source":["first_document_embedding = model.docvecs[0]\n","\n","print(first_document_embedding)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 0.0276915  -0.00424333 -0.03035437  0.03665333 -0.03213951 -0.01816239\n"," -0.07778365 -0.03633784 -0.02787157  0.03769598 -0.06721046  0.04989615\n","  0.03432883  0.01098605  0.01431738 -0.01388856  0.03482941  0.01128545\n"," -0.01953517 -0.02494158 -0.04504299  0.03002725  0.0299719   0.03134677\n","  0.0103391  -0.0370337   0.04888672 -0.06606048 -0.01375777  0.05624491\n","  0.05905925 -0.00221229  0.0647403   0.04874262 -0.0057706   0.01769597\n","  0.00062595  0.04845971  0.0154444  -0.00515356  0.0259825  -0.03847988\n","  0.03785087 -0.04696894  0.06054603 -0.01189699  0.04639918  0.01431327\n","  0.02038807 -0.03153087 -0.01505555 -0.02887575 -0.01999183  0.05477902\n","  0.05484637  0.03216729 -0.01537522  0.00627428 -0.00896889  0.01775825\n"," -0.04210246  0.03084033 -0.01253119 -0.07095121 -0.04444231  0.01873594\n","  0.07175679 -0.02109042  0.04794594  0.01523552  0.03240154  0.03453762\n"," -0.01690733 -0.04336994  0.04585202 -0.03524435  0.03358722 -0.05555914\n"," -0.04826734 -0.01969656  0.05696304 -0.00813756 -0.00090341  0.02600112\n"," -0.00762617 -0.00564663  0.05079151 -0.03786749 -0.01440831 -0.04273618\n"," -0.03693441  0.04302726 -0.01856985  0.01248481 -0.00252296 -0.02327968\n"," -0.02771858 -0.00896516  0.03861712 -0.00898274]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JVWTWeibkH-p"},"source":["Since we have now a vector representation for each document, we can look for the most similar document by calculating the cosine distance again. By using the same method as in the previous model, we find the document with the least distance to the first document. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cdzo_RO_PX60","executionInfo":{"elapsed":38986,"status":"ok","timestamp":1620313245795,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"},"user_tz":-120},"outputId":"be997cfd-4770-49cc-8dfd-353b3007abe3"},"source":["min_distance = 1.0\n","most_similar_index = 0\n","counter = 0\n","\n","for document in model.docvecs.vectors_docs:\n","  distance = cosine(first_document_embedding, document)\n","  if (distance < min_distance) and (distance != 0.0):\n","    min_distance = distance\n","    most_similar_index = counter\n","  counter += 1\n","\n","most_similar_index, min_distance"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(596, 0.09099602699279785)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"sYDKRTcrkqVz"},"source":["We then print the first document as well as its closest in the vector space. We can clearly see that both documents are similar, unlike with the previous approach. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dg55tdygUI9n","executionInfo":{"elapsed":39464,"status":"ok","timestamp":1620313246278,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"},"user_tz":-120},"outputId":"11d2c461-4f64-4e12-9bdf-baf9d4ff7f1b"},"source":["print(lemmatized_data['content'].iloc[0])\n","print(lemmatized_data['content'].iloc[most_similar_index])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["car wonder enlighten car see day door sport car look late early call bricklin door small addition bumper separate rest body know tellme model engine specs year production car history info funky looking car mail thank\n","car alarm info ungo box want car alarm think get ungo box knowledge experience alarm price range different model good car alarm email responce cak lehigh edu chad chad\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JOirV242k316"},"source":["We now convert our embeddings into a dataframe like we did before, where each row is a different document and the columns are the vector representation. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFULwJcTbkEh","executionInfo":{"elapsed":39460,"status":"ok","timestamp":1620313246279,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"},"user_tz":-120},"outputId":"6885a66b-4cb7-4fd4-dc4b-41473769359a"},"source":["embeddings_df = pd.DataFrame(model.docvecs.vectors_docs)\n","\n","print(embeddings_df.head())\n","print('\\nShape: {}'.format(embeddings_df.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["         0         1         2         3         4         5         6   \\\n","0  0.027692 -0.004243 -0.030354  0.036653 -0.032140 -0.018162 -0.077784   \n","1  0.027123  0.003297  0.003503  0.029629  0.005422  0.024020 -0.027331   \n","2  0.077605  0.061941  0.048014  0.027540  0.095746  0.014163  0.103565   \n","3  0.018683 -0.007956  0.014993 -0.002906 -0.051899  0.023039 -0.008452   \n","4  0.002189  0.001715 -0.009429  0.041566  0.018541  0.015386  0.005003   \n","\n","         7         8         9   ...        90        91        92        93  \\\n","0 -0.036338 -0.027872  0.037696  ... -0.036934  0.043027 -0.018570  0.012485   \n","1 -0.027808 -0.014980 -0.042095  ...  0.016356 -0.002130  0.012637 -0.001656   \n","2 -0.039304  0.006564 -0.047993  ... -0.028965  0.023155  0.000817  0.045025   \n","3 -0.023287 -0.020055 -0.002552  ...  0.033085 -0.024785  0.001221  0.006971   \n","4 -0.049475 -0.001632  0.014421  ...  0.015579  0.050467  0.037013  0.014271   \n","\n","         94        95        96        97        98        99  \n","0 -0.002523 -0.023280 -0.027719 -0.008965  0.038617 -0.008983  \n","1 -0.015569 -0.012326 -0.003447 -0.006032 -0.003334  0.040971  \n","2 -0.022006 -0.019142  0.072836 -0.003616 -0.030211 -0.021102  \n","3  0.010323 -0.001635  0.021323 -0.011600 -0.022501  0.012344  \n","4  0.018784 -0.051050  0.031416 -0.041880  0.024248  0.040299  \n","\n","[5 rows x 100 columns]\n","\n","Shape: (11314, 100)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"faVFjfJnlJ9_"},"source":["We then save our Doc2Vec model to a pickle file which we can use later. "]},{"cell_type":"code","metadata":{"id":"RLLhwsoCbzX8"},"source":["pickle.dump(embeddings_df, open('/content/drive/MyDrive/Colab Notebooks/TWSM Analytics Lab/storage/DoctoVecModel.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NIs_jx9Mld2u"},"source":["## Part 3. BERT"]},{"cell_type":"markdown","metadata":{"id":"ws_-2NrFlGwQ"},"source":["We check if our GPU is available and recognized by Tensorflow. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TvFHryKylgBS","executionInfo":{"status":"ok","timestamp":1620655567863,"user_tz":-120,"elapsed":5760,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"2e7740ee-9c08-4624-ddec-3a99a450e2b1"},"source":["device_name = tf.test.gpu_device_name()\n","\n","if device_name != '':\n","  print('Found GPU at: {}'.format(device_name))\n","else:\n","  raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z7yaJJfFlGwQ"},"source":["We then check if our GPU is also available and recognized by PyTorch. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vmfLVDfRk27e","executionInfo":{"status":"ok","timestamp":1620655567863,"user_tz":-120,"elapsed":5751,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"820ee061-d731-47c8-a63b-da75665999e3"},"source":["if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","  print('There are %d GPU(s) available.' %torch.cuda.device_count())\n","  print('We will use the GPU: ', torch.cuda.get_device_name(0))\n","else:\n","  print('No GPU available, using the CPU instead.')\n","  device = torch.device('cpu')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU:  Tesla K80\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Am8Uzy7VlGwQ"},"source":["We add the tags '[CLS]' and '[SEP]' at the beginning and at the end of each document respectively, so that the BERT model can identify the task it was pre-trained to do.  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_IeawLcvlnlX","executionInfo":{"status":"ok","timestamp":1620655567864,"user_tz":-120,"elapsed":5746,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"3fbcb734-451b-4fae-8318-09c2c78cbc6b"},"source":["sentences = ['[CLS] ' + query + ' [SEP]' for query in lemmatized_data['content']]\n","\n","print(sentences[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[CLS] car wonder enlighten car see day door sport car look late early call bricklin door small addition bumper separate rest body know tellme model engine specs year production car history info funky looking car mail thank [SEP]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xlbb3SpglGwR"},"source":["We define a new BertTokenizer object from a pre-trained model and then tokenize each document. We print the first one to see how the BertTokenizer splits the words."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201,"referenced_widgets":["92e90cb2be5a4a93bf71fe9cdf6a0f35","fac6b6fbe67249e29b758d9c88cd3387","b079faa8de0047a794934ddf3f7884b4","ff4ff8f34d7c4ce6998b8a63359df5a9","eb8413b73c0e43a0b7d74e232473cdcd","fffabd88a31f4d65b506393ff608f5be","96980f31973145728c11ad45160b5534","27b922c27c854b0d9a16fc49f39e9eb1","612de26177af4aadb2e58b8e433a45aa","ecaaa56039a14907aafbde0435841ebe","70f82cda582047e6a2017ae7a0c90be6","af274c7d57b3425c93eb79c9f587e8b6","5e9834da11f6424bb811f6827032ccd7","7c2baed9200d43ae88847954079656b6","84beac45c3724120b639841d81d2cd82","5f7bbf0b9d744126b844d0b2d101fc73","b57f7a3335a44b1db49ed487c7ae0478","ef6bb1ae04c342a299465f4663dcc465","3b74d7788c4f4ce6a497364c98fdf462","58d16f70ca0f4b84a8d9cda08f63946b","69f33c5793f14fa193c2156a2085c5b3","1c8b0938e65740e3946b8a05fe9a3cdf","cefc4df08efb4a2188b02dccaed2eea5","458f248e91514f3f9313f5f12d9f012f"]},"id":"NBBt6nDemXKJ","executionInfo":{"status":"ok","timestamp":1620655614634,"user_tz":-120,"elapsed":52511,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"217d5945-2dc7-4f31-fbda-147db3de4be3"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","tokenized_texts = [tokenizer.tokenize(sentence) for sentence in sentences]\n","\n","print(tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92e90cb2be5a4a93bf71fe9cdf6a0f35","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"612de26177af4aadb2e58b8e433a45aa","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b57f7a3335a44b1db49ed487c7ae0478","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","['[CLS]', 'car', 'wonder', 'en', '##light', '##en', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'brick', '##lin', 'door', 'small', 'addition', 'bumper', 'separate', 'rest', 'body', 'know', 'tell', '##me', 'model', 'engine', 'spec', '##s', 'year', 'production', 'car', 'history', 'info', 'funky', 'looking', 'car', 'mail', 'thank', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HrKI8WN8lGwR"},"source":["Here we want to see the size of the largest tokenization performed, where we can see that the number of tokens is quite large."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPjXnjOlnawj","executionInfo":{"status":"ok","timestamp":1620655614635,"user_tz":-120,"elapsed":52506,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"1e6a41a7-6fb0-4f57-ee91-9ce1400fdf9a"},"source":["max_length = max([len(document) for document in tokenized_texts])\n","\n","print(max_length)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["8232\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a8VO86tBlGwR"},"source":["We see that the number of tokens generated for each document vary a lot, so here we want to pad each sequence to make them have the same length. Since we have sequences with variable length strings, we set the parameter 'dtype' to 'object', as it is recommended. We then define the parameter 'maxlen' to 512, which means that the maximum length of each sequence will be increased or decreased to this number; we also chose 512, because that is the length of the tensors that the BERT Model is expecting. The 'value' parameter is set to '[PAD]', which means that all the spaces that we are filling will have this string. Finally, both the 'truncating' and 'padding' parameters are set to 'post', which means that the padding and truncating will be performed at the right side of the values. "]},{"cell_type":"code","metadata":{"id":"gnvH-myooDYo"},"source":["sentences_padded = pad_sequences(tokenized_texts, dtype=object, maxlen=512, \n","                                 value='[PAD]', truncating='post', padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9AS3pg97lGwS"},"source":["We now convert each of the tokens to a numerical id based on the vocabulary of the documents. "]},{"cell_type":"code","metadata":{"id":"szwCNVrLos4A"},"source":["sentences_converted = [tokenizer.convert_tokens_to_ids(token) for token in sentences_padded]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d-fLbxztlGwS"},"source":["We create mask for each of the sequences we created in the previous step, where we write a one if the id is higher than zero or zero if the id is lower than zero; in other words, the mask consists of ones and zeros depending on the value of the id given. "]},{"cell_type":"code","metadata":{"id":"Du61euXwp5cT"},"source":["masks = []\n","\n","for seq in sentences_converted:\n","  seq_mask = [int(i>0) for i in seq]\n","  masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KSCXdywblGwT"},"source":["We now create two tensors, one from the sentences converted to ids, which we will use as inputs, and another from the masks, which we will also use as masks. "]},{"cell_type":"code","metadata":{"id":"-zK5WLqnsWOu"},"source":["inputs = torch.LongTensor(sentences_converted)\n","masks = torch.LongTensor(masks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LehiR3qnlGwT"},"source":["We define a new BertModel from its pre-trained setup 'bert-case-uncased'. "]},{"cell_type":"code","metadata":{"id":"QA0uCAMLsjyc","colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["9f725096a2ae41d8abe96106bc8a2af8","b6f26d88ecec437094c20a40e8ad61ad","bcd5368ee1064281a91365c04ed91a12","7f94638b6e124e32a51a3c349a417718","e00e4af94e5246c7825528826c63858d","6e31be56a7b6420abbfc8cc760f7da32","b94e7b1063d9458f8c58256dfaa8ff27","48cad35fca264b4bb73380919f9107fb","ba0b2d2543994ceb87ebd8249d0d26d1","c42f1d509d454550b577dc61b00dafd2","e8dcd3eac8da4e5e941025ae06a549be","a74d5bbddd3c4223a0fa3ef2a152f48c","8120dd3346864259bbf360f09988e1c1","0e18db08afcc4526a05b62785840c8b4","411cc332b39540f5b3c518c3b5ca7b83","c4e917dabd954958b33096f5543920f9"]},"executionInfo":{"status":"ok","timestamp":1620655653690,"user_tz":-120,"elapsed":91553,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"41fb7280-8c48-43a1-abb1-c6ddce1e510c"},"source":["model = BertModel.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f725096a2ae41d8abe96106bc8a2af8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba0b2d2543994ceb87ebd8249d0d26d1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K2rqEp6flGwT"},"source":["We then send it to the device that we will use to train our model, which in our case, it's the GPU. We can also see the architecture of the neural network that the model uses."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJsF-i48lGwT","executionInfo":{"status":"ok","timestamp":1620655662656,"user_tz":-120,"elapsed":100510,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"4c2f5c60-472b-4c8a-d934-f024b23884a1"},"source":["model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"0vztk-AqlGwU"},"source":["We now create a TensorDataset using the inputs and masks that we created before. We also define a SequentialSampler, which will take samples from our dataset. Finally, we create a DataLoader with our dataset, sampler, and using a batch size of 16. "]},{"cell_type":"code","metadata":{"id":"0AHvR_PKlGwU"},"source":["batch_size = 16\n","prediction_data = TensorDataset(inputs, masks)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTJxRFnulGwU"},"source":["We now loop through the dataloader, which gives batches to the model to be trained with, and we then append the resulting embeddings given by the model to our list of results."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDWB4nzQlGwU","executionInfo":{"status":"ok","timestamp":1620656390193,"user_tz":-120,"elapsed":828040,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"561de484-5cf4-43ab-96d2-121d24cd1250"},"source":["result = []\n","i = 0\n","\n","for batch in prediction_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    b_input_ids, b_input_mask = batch\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","    embeddings = outputs.pooler_output # CLS embeddings for the batch\n","    embeddings = embeddings.detach().cpu().numpy()\n","    result.append(embeddings)\n","    i = i + 1\n","print('DONE')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["DONE\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oZZiiDuqvkMw"},"source":["Here we print the length of our result and the shape of each batch to understand how it looks like. We had 11314 documents and we fed the data in batches of 16 documents, which result in 708 total batches to process all the documents. This is also why the first value of the shape of the batch is 16 and the second value is 768, because that's the size of the embedding that the BERT model outputs. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlwsY73tvCT4","executionInfo":{"status":"ok","timestamp":1620657943379,"user_tz":-120,"elapsed":892,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"2b09a183-d354-4c47-86d0-87e884785a2c"},"source":["print('Length of Result: {}'.format(len(result)))\n","print('Batch Shape: {}'.format(result[0].shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Length of Result: 708\n","Batch Shape: (16, 768)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CUNAi22DlGwU"},"source":["We loop through each batch extracting each embedding to then create a dataframe with all of them. We then print the head of the dataframe to understand how it looks. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6r3ohO_vlGwV","executionInfo":{"status":"ok","timestamp":1620657272371,"user_tz":-120,"elapsed":4170,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"e596e144-9482-463f-c2c6-a336adc78347"},"source":["final = []\n","\n","for batch in result:\n","   for embedding in batch:\n","      final.append(embedding)\n","\n","final_df = pd.DataFrame(final)\n","\n","print(final_df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["        0         1         2    ...       765       766       767\n","0 -0.127447 -0.267551 -0.952460  ... -0.817432 -0.201459 -0.155924\n","1 -0.128594 -0.297495 -0.960266  ... -0.858595 -0.153644 -0.225011\n","2 -0.364291 -0.418737 -0.972758  ... -0.922895 -0.341115 -0.155387\n","3 -0.208075 -0.404881 -0.984882  ... -0.915685 -0.226392 -0.209980\n","4 -0.134173 -0.371447 -0.981081  ... -0.921013 -0.209249 -0.286874\n","\n","[5 rows x 768 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oneW-7xrlGwV"},"source":["Finally, we save our results to a pickle file, so we can use this data later on."]},{"cell_type":"code","metadata":{"id":"bYTJ2JJslGwV"},"source":["pickle.dump(final_df, open('/content/drive/MyDrive/Colab Notebooks/TWSM Analytics Lab/storage/BertModel.pkl', 'wb'))"],"execution_count":null,"outputs":[]}]}