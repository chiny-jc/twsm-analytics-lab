{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_TextRepresentation.ipynb","provenance":[],"mount_file_id":"1Rdkr-zTOaIMNU2lUYlAXmfZK7j3-YS0m","authorship_tag":"ABX9TyM4B9y3g6uL0xCXo2cRWK8m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pnZs74MnOa-S"},"source":["# Exercise 3. Text Representation\n","### Text, Web and Social Media Analytics"]},{"cell_type":"markdown","metadata":{"id":"_U9PQR6qOrbk"},"source":["In this exercise, we will derive the document representaion of the preprocessed (stemmed) newsgroups dataset. We will use sklearn, as well as gensim to derive the bag of words document representation. We will calculate the following representations for each package: \n","\n","- Absolute frequencies\n","- Relative frequencies\n","- TF-IDF frequencies\n","- N-grams"]},{"cell_type":"markdown","metadata":{"id":"b3HnC99GPOR3"},"source":["We first import all the libraries we will be using."]},{"cell_type":"code","metadata":{"id":"x9h0UhTzNLRM"},"source":["import pickle\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.preprocessing import Binarizer\n","from gensim.corpora import Dictionary\n","from gensim.models import TfidfModel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZOGl6xv3PSX2"},"source":["We now load the preprocessed dataset from the previous exercise by using the pickle package. We then print the first row to make sure it was loaded correctly. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c22x1JL2OKvf","executionInfo":{"status":"ok","timestamp":1620024008762,"user_tz":-120,"elapsed":3299,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"7f03b490-73a5-4d65-fa57-c14354170de8"},"source":["stemmed_data = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/TWSM Analytics Lab/storage/stemmed_data.p', 'rb'))\n","\n","print(stemmed_data.iloc[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["content         car wonder enlighten car saw dai door sport ca...\n","target                                                          7\n","target_names                                            rec.autos\n","Name: 0, dtype: object\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xPYTDREBQTFI"},"source":["## Bag-of-Words in Scikit-Learn\n","\n","Here, we define a CountVectorizer object, which calculates the absolute term frequency; this means that the value of each word is the number of appearances in the document. We also define two parameters, 'max_df' and 'min_df'. These two parameters are set to 0.95 and 0.05 respectively, which specifies to leave out any vocabulary that has a document frequency higher than 95% and lower than 5%.  \n","\n","After this, we fit and transform our data to generate a matrix representation of our text. We then print the features in the matrix, as well as the shape of the matrix and the first few values to get an idea of how the matrix looks."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ay3hwQdOQJ0q","executionInfo":{"status":"ok","timestamp":1620024010285,"user_tz":-120,"elapsed":4818,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"86fbadef-48bf-46aa-8aa3-80186f9b4b9d"},"source":["count_vectorizer = CountVectorizer(max_df=0.95, min_df=0.05)\n","count_vectorizer_matrix = count_vectorizer.fit_transform(stemmed_data['content'])\n","\n","print('Features:\\n{}\\n'.format(count_vectorizer.get_feature_names()))\n","print('Matrix Shape:\\n{}\\n'.format(count_vectorizer_matrix.shape))\n","print('First Values:\\n{}'.format(count_vectorizer_matrix[:5, :5].todense()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Features:\n","['abl', 'accept', 'actual', 'address', 'advanc', 'ago', 'agre', 'allow', 'american', 'answer', 'anybodi', 'appreci', 'apr', 'area', 'articl', 'ask', 'assum', 'avail', 'awai', 'bad', 'base', 'believ', 'best', 'better', 'big', 'bit', 'book', 'bui', 'call', 'car', 'card', 'care', 'case', 'caus', 'chang', 'check', 'chip', 'christian', 'claim', 'close', 'com', 'come', 'complet', 'consid', 'control', 'cost', 'cours', 'current', 'dai', 'data', 'david', 'deal', 'design', 'differ', 'discuss', 'drive', 'edu', 'effect', 'email', 'end', 'engin', 'exampl', 'exist', 'expect', 'experi', 'fact', 'far', 'fax', 'feel', 'file', 'final', 'follow', 'forc', 'free', 'game', 'gener', 'get', 'given', 'go', 'god', 'good', 'got', 'govern', 'great', 'group', 'guess', 'gui', 'hand', 'happen', 'hard', 'have', 'heard', 'help', 'high', 'home', 'hope', 'human', 'idea', 'import', 'includ', 'info', 'inform', 'interest', 'internet', 'isn', 'issu', 'john', 'kei', 'kill', 'kind', 'know', 'larg', 'law', 'left', 'let', 'life', 'like', 'line', 'list', 'littl', 'live', 'local', 'long', 'look', 'lot', 'love', 'machin', 'mail', 'major', 'make', 'man', 'mark', 'matter', 'mayb', 'mean', 'mention', 'messag', 'mind', 'nation', 'need', 'net', 'new', 'non', 'note', 'number', 'old', 'open', 'opinion', 'order', 'origin', 'peopl', 'person', 'phone', 'place', 'plai', 'point', 'posit', 'possibl', 'post', 'power', 'pretti', 'price', 'probabl', 'problem', 'program', 'provid', 'public', 'question', 'read', 'real', 'reason', 'recent', 'refer', 'rememb', 'report', 'requir', 'research', 'respons', 'result', 'right', 'run', 'sai', 'said', 'sale', 'second', 'seen', 'send', 'set', 'small', 'softwar', 'sound', 'sourc', 'space', 'standard', 'start', 'state', 'stuff', 'suggest', 'support', 'sure', 'system', 'take', 'talk', 'team', 'tell', 'thank', 'thing', 'think', 'thought', 'time', 'todai', 'true', 'try', 'turn', 'type', 'understand', 'univers', 'us', 'usual', 'version', 'view', 'wai', 'want', 'week', 'win', 'window', 'won', 'wonder', 'word', 'work', 'world', 'write', 'wrong', 'wrote', 'ye', 'year']\n","\n","Matrix Shape:\n","(11314, 236)\n","\n","First Values:\n","[[0 0 0 0 0]\n"," [0 0 0 0 0]\n"," [0 0 1 0 1]\n"," [0 0 0 1 0]\n"," [0 0 0 0 0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ss7Q83yaU-e0"},"source":["Now, we define a TfidfVectorizer object, which will calculate the relative term frequency of each word; this means the number of times that a word appears in the document is divided by the length of the document. To do this, we define two parameters, 'use_idf' to false, so the inverse-document-frequency reweighting is not calculated, and 'norm' to 'l1', so the value of all the words in the document sums to one. We also set the parameters 'max_df' and 'min_df' like we did last time. \n","\n","We then print the feature names, the matrix shape and the first few values to get an idea of how the matrix looks."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uikECLIXbHS-","executionInfo":{"status":"ok","timestamp":1620024011789,"user_tz":-120,"elapsed":6316,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"621b5e4b-a53a-4964-abc0-dc84c0b73c21"},"source":["tfidf_vectorizer_l1 = TfidfVectorizer(max_df=0.95, min_df=0.05, use_idf=False, norm='l1')\n","tfidf_vectorizer_l1_matrix = tfidf_vectorizer_l1.fit_transform(stemmed_data['content'])\n","\n","print('Features:\\n{}\\n'.format(tfidf_vectorizer_l1.get_feature_names()))\n","print('Matrix Shape:\\n{}\\n'.format(tfidf_vectorizer_l1_matrix.shape))\n","print('First Values:\\n{}'.format(tfidf_vectorizer_l1_matrix[:5, :5].todense()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Features:\n","['abl', 'accept', 'actual', 'address', 'advanc', 'ago', 'agre', 'allow', 'american', 'answer', 'anybodi', 'appreci', 'apr', 'area', 'articl', 'ask', 'assum', 'avail', 'awai', 'bad', 'base', 'believ', 'best', 'better', 'big', 'bit', 'book', 'bui', 'call', 'car', 'card', 'care', 'case', 'caus', 'chang', 'check', 'chip', 'christian', 'claim', 'close', 'com', 'come', 'complet', 'consid', 'control', 'cost', 'cours', 'current', 'dai', 'data', 'david', 'deal', 'design', 'differ', 'discuss', 'drive', 'edu', 'effect', 'email', 'end', 'engin', 'exampl', 'exist', 'expect', 'experi', 'fact', 'far', 'fax', 'feel', 'file', 'final', 'follow', 'forc', 'free', 'game', 'gener', 'get', 'given', 'go', 'god', 'good', 'got', 'govern', 'great', 'group', 'guess', 'gui', 'hand', 'happen', 'hard', 'have', 'heard', 'help', 'high', 'home', 'hope', 'human', 'idea', 'import', 'includ', 'info', 'inform', 'interest', 'internet', 'isn', 'issu', 'john', 'kei', 'kill', 'kind', 'know', 'larg', 'law', 'left', 'let', 'life', 'like', 'line', 'list', 'littl', 'live', 'local', 'long', 'look', 'lot', 'love', 'machin', 'mail', 'major', 'make', 'man', 'mark', 'matter', 'mayb', 'mean', 'mention', 'messag', 'mind', 'nation', 'need', 'net', 'new', 'non', 'note', 'number', 'old', 'open', 'opinion', 'order', 'origin', 'peopl', 'person', 'phone', 'place', 'plai', 'point', 'posit', 'possibl', 'post', 'power', 'pretti', 'price', 'probabl', 'problem', 'program', 'provid', 'public', 'question', 'read', 'real', 'reason', 'recent', 'refer', 'rememb', 'report', 'requir', 'research', 'respons', 'result', 'right', 'run', 'sai', 'said', 'sale', 'second', 'seen', 'send', 'set', 'small', 'softwar', 'sound', 'sourc', 'space', 'standard', 'start', 'state', 'stuff', 'suggest', 'support', 'sure', 'system', 'take', 'talk', 'team', 'tell', 'thank', 'thing', 'think', 'thought', 'time', 'todai', 'true', 'try', 'turn', 'type', 'understand', 'univers', 'us', 'usual', 'version', 'view', 'wai', 'want', 'week', 'win', 'window', 'won', 'wonder', 'word', 'work', 'world', 'write', 'wrong', 'wrote', 'ye', 'year']\n","\n","Matrix Shape:\n","(11314, 236)\n","\n","First Values:\n","[[0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.01785714 0.         0.01785714]\n"," [0.         0.         0.         0.04166667 0.        ]\n"," [0.         0.         0.         0.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EoUEfPOyXlWp"},"source":["We now define another TfidVectorizer object, which will calculate the actual TF-IDF frequency; this means that the frequency of each word is multiplied by the logarithm of the total number of documents divided by the total number of documents that contain that word. We set the 'max_df' and 'min_df' parameters like before, and also set 'smooth_idf' to false, so an extra document with all words is not used for value smoothing. \n","\n","Then we also print the feature names, the matrix shape and the first few values."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLsD86pLbI6j","executionInfo":{"status":"ok","timestamp":1620024012887,"user_tz":-120,"elapsed":7410,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"297b5331-5f50-4aff-9d81-1b28d2a4b69b"},"source":["tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=0.05, smooth_idf=False)\n","tfidf_vectorizer_matrix = tfidf_vectorizer.fit_transform(stemmed_data['content'])\n","\n","print('Features:\\n{}\\n'.format(tfidf_vectorizer.get_feature_names()))\n","print('Matrix Shape:\\n{}\\n'.format(tfidf_vectorizer_matrix.shape))\n","print('First Values:\\n{}'.format(tfidf_vectorizer_matrix[:5, :5].todense()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Features:\n","['abl', 'accept', 'actual', 'address', 'advanc', 'ago', 'agre', 'allow', 'american', 'answer', 'anybodi', 'appreci', 'apr', 'area', 'articl', 'ask', 'assum', 'avail', 'awai', 'bad', 'base', 'believ', 'best', 'better', 'big', 'bit', 'book', 'bui', 'call', 'car', 'card', 'care', 'case', 'caus', 'chang', 'check', 'chip', 'christian', 'claim', 'close', 'com', 'come', 'complet', 'consid', 'control', 'cost', 'cours', 'current', 'dai', 'data', 'david', 'deal', 'design', 'differ', 'discuss', 'drive', 'edu', 'effect', 'email', 'end', 'engin', 'exampl', 'exist', 'expect', 'experi', 'fact', 'far', 'fax', 'feel', 'file', 'final', 'follow', 'forc', 'free', 'game', 'gener', 'get', 'given', 'go', 'god', 'good', 'got', 'govern', 'great', 'group', 'guess', 'gui', 'hand', 'happen', 'hard', 'have', 'heard', 'help', 'high', 'home', 'hope', 'human', 'idea', 'import', 'includ', 'info', 'inform', 'interest', 'internet', 'isn', 'issu', 'john', 'kei', 'kill', 'kind', 'know', 'larg', 'law', 'left', 'let', 'life', 'like', 'line', 'list', 'littl', 'live', 'local', 'long', 'look', 'lot', 'love', 'machin', 'mail', 'major', 'make', 'man', 'mark', 'matter', 'mayb', 'mean', 'mention', 'messag', 'mind', 'nation', 'need', 'net', 'new', 'non', 'note', 'number', 'old', 'open', 'opinion', 'order', 'origin', 'peopl', 'person', 'phone', 'place', 'plai', 'point', 'posit', 'possibl', 'post', 'power', 'pretti', 'price', 'probabl', 'problem', 'program', 'provid', 'public', 'question', 'read', 'real', 'reason', 'recent', 'refer', 'rememb', 'report', 'requir', 'research', 'respons', 'result', 'right', 'run', 'sai', 'said', 'sale', 'second', 'seen', 'send', 'set', 'small', 'softwar', 'sound', 'sourc', 'space', 'standard', 'start', 'state', 'stuff', 'suggest', 'support', 'sure', 'system', 'take', 'talk', 'team', 'tell', 'thank', 'thing', 'think', 'thought', 'time', 'todai', 'true', 'try', 'turn', 'type', 'understand', 'univers', 'us', 'usual', 'version', 'view', 'wai', 'want', 'week', 'win', 'window', 'won', 'wonder', 'word', 'work', 'world', 'write', 'wrong', 'wrote', 'ye', 'year']\n","\n","Matrix Shape:\n","(11314, 236)\n","\n","First Values:\n","[[0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.10243341 0.         0.11856047]\n"," [0.         0.         0.         0.23425716 0.        ]\n"," [0.         0.         0.         0.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qB2ZyQ1hP1CC"},"source":["Here, we define a Binarizer object, which performs a one-hot encoding on the matrix that we had from the CountVectorizer. This means that the values for each word represent if the word is present or not in the document by using ones and zeros. \n","\n","We then print the shape and first few values to confirm that the encoding is working properly. We can see that now all values different than zero are replaced by ones. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSB14xyxPOxN","executionInfo":{"status":"ok","timestamp":1620024012888,"user_tz":-120,"elapsed":7405,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"96fc0374-c164-476d-93bb-b7a86dafba8a"},"source":["binarizer = Binarizer()\n","binarizer_matrix = binarizer.fit_transform(count_vectorizer_matrix)\n","\n","print('Matrix Shape:\\n{}\\n'.format(binarizer_matrix.shape))\n","print('First Values:\\n{}'.format(binarizer_matrix[:5, :5].todense()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Matrix Shape:\n","(11314, 236)\n","\n","First Values:\n","[[0 0 0 0 0]\n"," [0 0 0 0 0]\n"," [0 0 1 0 1]\n"," [0 0 0 1 0]\n"," [0 0 0 0 0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"klGi5k2WdC1d"},"source":["We now want to create a quick comparison of the first three methods shown above for the first document. We remove all the columns that have words that do not appear in the document and append all the rows into a single dataframe. We can now see how the values for each word changes from method to method. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"9df_j6S7SLbP","executionInfo":{"status":"ok","timestamp":1620024012889,"user_tz":-120,"elapsed":7401,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"00384708-ab8e-44fb-fa9a-242741e061f7"},"source":["first_doc_cv = pd.DataFrame(count_vectorizer_matrix[0].todense(), columns=count_vectorizer.get_feature_names()).rename(index={0: \"Abs. Freq.\"})\n","first_doc_tfid_l1 = pd.DataFrame(tfidf_vectorizer_l1_matrix[0].todense(), columns=tfidf_vectorizer_l1.get_feature_names()).rename(index={0: \"Rel. Freq.\"})\n","first_doc_tfid = pd.DataFrame(tfidf_vectorizer_matrix[0].todense(), columns=tfidf_vectorizer.get_feature_names()).rename(index={0: \"TF-IDF\"})\n","first_doc_bin = pd.DataFrame(binarizer_matrix[0].todense(), columns=count_vectorizer.get_feature_names()).rename(index={0: \"One-Hot\"})\n","first_doc_df = first_doc_cv.append(first_doc_tfid_l1).append(first_doc_tfid).append(first_doc_bin)\n","first_doc_df = first_doc_df.loc[:, (first_doc_df != 0).all()]\n","first_doc_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>call</th>\n","      <th>car</th>\n","      <th>dai</th>\n","      <th>engin</th>\n","      <th>info</th>\n","      <th>know</th>\n","      <th>look</th>\n","      <th>mail</th>\n","      <th>small</th>\n","      <th>thank</th>\n","      <th>wonder</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Abs. Freq.</th>\n","      <td>1.000000</td>\n","      <td>5.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Rel. Freq.</th>\n","      <td>0.058824</td>\n","      <td>0.294118</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.117647</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","    </tr>\n","    <tr>\n","      <th>TF-IDF</th>\n","      <td>0.148914</td>\n","      <td>0.850723</td>\n","      <td>0.140981</td>\n","      <td>0.171929</td>\n","      <td>0.173160</td>\n","      <td>0.097393</td>\n","      <td>0.236555</td>\n","      <td>0.139354</td>\n","      <td>0.177220</td>\n","      <td>0.121796</td>\n","      <td>0.169629</td>\n","      <td>0.121642</td>\n","    </tr>\n","    <tr>\n","      <th>One-Hot</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                call       car       dai  ...     thank    wonder      year\n","Abs. Freq.  1.000000  5.000000  1.000000  ...  1.000000  1.000000  1.000000\n","Rel. Freq.  0.058824  0.294118  0.058824  ...  0.058824  0.058824  0.058824\n","TF-IDF      0.148914  0.850723  0.140981  ...  0.121796  0.169629  0.121642\n","One-Hot     1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000\n","\n","[4 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"31pqojddesQP"},"source":["## Bag-of-Words using Gensim"]},{"cell_type":"markdown","metadata":{"id":"3oU7g4nAx8IB"},"source":["First, we tokenize each document and create a list of words for all of the documents, resulting with a list of lists of words."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLScGmOQe0cV","executionInfo":{"status":"ok","timestamp":1620024013638,"user_tz":-120,"elapsed":8145,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"913b6b04-aaf3-48ad-ef84-3410ce5981b9"},"source":["corpus_gen = [doc.split() for doc in stemmed_data['content']]\n","\n","print(corpus_gen[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['car', 'wonder', 'enlighten', 'car', 'saw', 'dai', 'door', 'sport', 'car', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'car', 'histori', 'info', 'funki', 'look', 'car', 'mail', 'thank']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CMGfVmw8_f7G"},"source":["Now, we create a Dictionary object where we give it the list of lists that we generated in the previous step. We also define two parameters, 'no_below' and 'no_above', which mean the minimum number of documents and the maximum percentage of documents, that we want a word to be in to be considered in the dictionary. We then print the result to understand how it looks like."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CDOHLxvnym1n","executionInfo":{"status":"ok","timestamp":1620024014841,"user_tz":-120,"elapsed":9341,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"08d38484-c17d-4c40-f750-a2ae20197e42"},"source":["id2word = Dictionary(corpus_gen)\n","id2word.filter_extremes(no_below=566, no_above=0.95,)\n","\n","print(id2word)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dictionary(236 unique tokens: ['call', 'car', 'dai', 'engin', 'info']...)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qHIh9UMaAdkl"},"source":["To understand better how the Dictionary object works, we print some of its attributes. We first print the actual dictionary of key-value pairs, having the word and its id, as well as all the words considered in the dictionary and, finally, the number of documents each word appears in according to its id. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"huZ9ApFDzWdp","executionInfo":{"status":"ok","timestamp":1620024014842,"user_tz":-120,"elapsed":9335,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"6817868f-ef64-4762-f757-3e25b4717594"},"source":["print(id2word.token2id)\n","print(id2word.token2id.keys())\n","print(id2word.dfs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'call': 0, 'car': 1, 'dai': 2, 'engin': 3, 'info': 4, 'know': 5, 'look': 6, 'mail': 7, 'small': 8, 'thank': 9, 'wonder': 10, 'year': 11, 'answer': 12, 'base': 13, 'card': 14, 'edu': 15, 'experi': 16, 'final': 17, 'gui': 18, 'messag': 19, 'number': 20, 'report': 21, 'send': 22, 'actual': 23, 'advanc': 24, 'anybodi': 25, 'better': 26, 'bit': 27, 'expect': 28, 'feel': 29, 'good': 30, 'got': 31, 'great': 32, 'heard': 33, 'help': 34, 'life': 35, 'like': 36, 'line': 37, 'machin': 38, 'mayb': 39, 'new': 40, 'opinion': 41, 'peopl': 42, 'plai': 43, 'price': 44, 'probabl': 45, 'question': 46, 'read': 47, 'real': 48, 'recent': 49, 'start': 50, 'take': 51, 'time': 52, 'us': 53, 'wai': 54, 'address': 55, 'articl': 56, 'chip': 57, 'com': 58, 'far': 59, 'inform': 60, 'person': 61, 'phone': 62, 'point': 63, 'pretti': 64, 'requir': 65, 'stuff': 66, 'system': 67, 'thing': 68, 'write': 69, 'wrote': 70, 'mean': 71, 'possibl': 72, 'right': 73, 'set': 74, 'tell': 75, 'understand': 76, 'world': 77, 'ye': 78, 'agre': 79, 'allow': 80, 'apr': 81, 'believ': 82, 'check': 83, 'come': 84, 'consid': 85, 'control': 86, 'cost': 87, 'cours': 88, 'exist': 89, 'follow': 90, 'given': 91, 'govern': 92, 'hand': 93, 'hard': 94, 'hope': 95, 'idea': 96, 'john': 97, 'kill': 98, 'make': 99, 'need': 100, 'non': 101, 'power': 102, 'reason': 103, 'result': 104, 'sai': 105, 'second': 106, 'state': 107, 'support': 108, 'todai': 109, 'email': 110, 'file': 111, 'sure': 112, 'thought': 113, 'try': 114, 'accept': 115, 'avail': 116, 'data': 117, 'fact': 118, 'go': 119, 'list': 120, 'long': 121, 'love': 122, 'note': 123, 'post': 124, 'problem': 125, 'refer': 126, 'said': 127, 'standard': 128, 'think': 129, 'true': 130, 'version': 131, 'appreci': 132, 'chang': 133, 'win': 134, 'bui': 135, 'design': 136, 'mention': 137, 'usual': 138, 'work': 139, 'wrong': 140, 'run': 141, 'want': 142, 'assum': 143, 'case': 144, 'christian': 145, 'david': 146, 'differ': 147, 'end': 148, 'exampl': 149, 'get': 150, 'god': 151, 'guess': 152, 'human': 153, 'kind': 154, 'littl': 155, 'live': 156, 'man': 157, 'mind': 158, 'type': 159, 'current': 160, 'forc': 161, 'kei': 162, 'let': 163, 'major': 164, 'mark': 165, 'place': 166, 'provid': 167, 'softwar': 168, 'sourc': 169, 'space': 170, 'team': 171, 'ask': 172, 'high': 173, 'includ': 174, 'old': 175, 'sale': 176, 'sound': 177, 'caus': 178, 'gener': 179, 'group': 180, 'happen': 181, 'origin': 182, 'abl': 183, 'big': 184, 'complet': 185, 'deal': 186, 'import': 187, 'program': 188, 'won': 189, 'word': 190, 'ago': 191, 'awai': 192, 'best': 193, 'book': 194, 'care': 195, 'claim': 196, 'close': 197, 'drive': 198, 'interest': 199, 'law': 200, 'rememb': 201, 'respons': 202, 'turn': 203, 'univers': 204, 'view': 205, 'seen': 206, 'matter': 207, 'window': 208, 'order': 209, 'fax': 210, 'free': 211, 'left': 212, 'isn': 213, 'game': 214, 'lot': 215, 'local': 216, 'larg': 217, 'week': 218, 'effect': 219, 'public': 220, 'have': 221, 'american': 222, 'issu': 223, 'open': 224, 'talk': 225, 'home': 226, 'bad': 227, 'net': 228, 'posit': 229, 'area': 230, 'nation': 231, 'research': 232, 'discuss': 233, 'suggest': 234, 'internet': 235}\n","dict_keys(['call', 'car', 'dai', 'engin', 'info', 'know', 'look', 'mail', 'small', 'thank', 'wonder', 'year', 'answer', 'base', 'card', 'edu', 'experi', 'final', 'gui', 'messag', 'number', 'report', 'send', 'actual', 'advanc', 'anybodi', 'better', 'bit', 'expect', 'feel', 'good', 'got', 'great', 'heard', 'help', 'life', 'like', 'line', 'machin', 'mayb', 'new', 'opinion', 'peopl', 'plai', 'price', 'probabl', 'question', 'read', 'real', 'recent', 'start', 'take', 'time', 'us', 'wai', 'address', 'articl', 'chip', 'com', 'far', 'inform', 'person', 'phone', 'point', 'pretti', 'requir', 'stuff', 'system', 'thing', 'write', 'wrote', 'mean', 'possibl', 'right', 'set', 'tell', 'understand', 'world', 'ye', 'agre', 'allow', 'apr', 'believ', 'check', 'come', 'consid', 'control', 'cost', 'cours', 'exist', 'follow', 'given', 'govern', 'hand', 'hard', 'hope', 'idea', 'john', 'kill', 'make', 'need', 'non', 'power', 'reason', 'result', 'sai', 'second', 'state', 'support', 'todai', 'email', 'file', 'sure', 'thought', 'try', 'accept', 'avail', 'data', 'fact', 'go', 'list', 'long', 'love', 'note', 'post', 'problem', 'refer', 'said', 'standard', 'think', 'true', 'version', 'appreci', 'chang', 'win', 'bui', 'design', 'mention', 'usual', 'work', 'wrong', 'run', 'want', 'assum', 'case', 'christian', 'david', 'differ', 'end', 'exampl', 'get', 'god', 'guess', 'human', 'kind', 'littl', 'live', 'man', 'mind', 'type', 'current', 'forc', 'kei', 'let', 'major', 'mark', 'place', 'provid', 'softwar', 'sourc', 'space', 'team', 'ask', 'high', 'includ', 'old', 'sale', 'sound', 'caus', 'gener', 'group', 'happen', 'origin', 'abl', 'big', 'complet', 'deal', 'import', 'program', 'won', 'word', 'ago', 'awai', 'best', 'book', 'care', 'claim', 'close', 'drive', 'interest', 'law', 'rememb', 'respons', 'turn', 'univers', 'view', 'seen', 'matter', 'window', 'order', 'fax', 'free', 'left', 'isn', 'game', 'lot', 'local', 'larg', 'week', 'effect', 'public', 'have', 'american', 'issu', 'open', 'talk', 'home', 'bad', 'net', 'posit', 'area', 'nation', 'research', 'discuss', 'suggest', 'internet'])\n","{1: 692, 10: 700, 2: 1326, 6: 2200, 0: 1111, 8: 591, 5: 3505, 3: 665, 11: 2041, 4: 647, 7: 1375, 9: 2034, 17: 574, 21: 594, 20: 1165, 16: 613, 22: 737, 19: 577, 14: 698, 13: 891, 12: 765, 18: 594, 15: 5649, 46: 1765, 50: 1222, 35: 717, 54: 2182, 40: 2923, 38: 605, 27: 992, 39: 791, 25: 567, 28: 573, 33: 722, 44: 604, 37: 971, 36: 3830, 49: 625, 45: 1109, 31: 1209, 29: 703, 26: 1203, 32: 946, 30: 2311, 41: 1043, 42: 2541, 53: 2501, 51: 635, 48: 901, 43: 764, 23: 1094, 34: 1613, 24: 647, 47: 1477, 52: 2789, 70: 810, 69: 6063, 56: 4991, 57: 581, 59: 951, 66: 566, 64: 649, 65: 728, 63: 1551, 55: 633, 62: 714, 60: 1105, 58: 3697, 67: 597, 68: 2054, 61: 1221, 77: 1001, 78: 761, 71: 1471, 76: 729, 73: 1845, 74: 985, 72: 1199, 75: 1198, 106: 861, 96: 946, 81: 2680, 97: 728, 102: 956, 99: 1099, 87: 584, 100: 2141, 86: 920, 92: 840, 104: 673, 101: 734, 89: 828, 107: 1349, 84: 1654, 82: 1401, 94: 838, 108: 957, 79: 660, 93: 779, 95: 759, 83: 645, 88: 1022, 105: 1280, 90: 1218, 98: 650, 103: 1289, 80: 735, 91: 673, 85: 965, 109: 697, 110: 769, 113: 931, 112: 1378, 114: 1615, 111: 816, 115: 663, 121: 1131, 125: 1826, 122: 568, 120: 750, 128: 639, 131: 586, 123: 812, 117: 626, 118: 1122, 124: 1782, 116: 856, 127: 1302, 129: 2935, 130: 875, 119: 1319, 126: 682, 134: 636, 133: 919, 132: 674, 136: 572, 139: 2298, 140: 817, 138: 573, 137: 632, 135: 709, 141: 1370, 142: 2325, 146: 795, 148: 1016, 147: 1332, 151: 839, 144: 1222, 157: 741, 145: 690, 158: 623, 156: 994, 149: 771, 154: 772, 155: 1104, 153: 582, 152: 585, 143: 630, 159: 658, 150: 1054, 170: 608, 168: 821, 165: 581, 163: 1321, 160: 788, 171: 573, 162: 636, 167: 754, 161: 578, 169: 657, 166: 1024, 164: 640, 176: 582, 173: 811, 177: 693, 174: 1118, 172: 1186, 175: 880, 180: 941, 181: 980, 182: 986, 178: 763, 179: 1137, 188: 992, 184: 740, 190: 837, 185: 604, 183: 821, 187: 616, 189: 698, 186: 566, 197: 620, 198: 896, 204: 1270, 201: 683, 203: 734, 196: 754, 200: 787, 192: 739, 191: 679, 199: 1127, 194: 657, 202: 763, 205: 634, 193: 994, 195: 634, 206: 781, 208: 1024, 207: 625, 209: 818, 211: 698, 210: 582, 212: 635, 213: 798, 214: 772, 215: 1162, 216: 570, 217: 643, 218: 655, 219: 706, 220: 744, 221: 833, 222: 607, 225: 922, 224: 596, 223: 720, 226: 636, 227: 785, 228: 639, 229: 568, 230: 621, 231: 680, 232: 604, 233: 629, 234: 764, 235: 670}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5ym_7SHfBSpS"},"source":["Here, we create the bag of words were we take into account the absolute frequency of each word for each document, so basically it tells us how many times a certain word appears in a certain document. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xssxc0v40Nqy","executionInfo":{"status":"ok","timestamp":1620024015629,"user_tz":-120,"elapsed":10115,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"adf5b8d8-f4d3-44b7-b45d-e1de1d22fe62"},"source":["corpus1 = [id2word.doc2bow(doc) for doc in corpus_gen]\n","\n","print(corpus1[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(0, 1), (1, 5), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bHbdU0bZBfJf"},"source":["Here, we take the bag of words we created previously and calculate the relative frequency for each word. To do this, we take the absolute frequency from the previous bag of words and divide it into the total number of words in the document. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DbKL1URe1CKX","executionInfo":{"status":"ok","timestamp":1620024016400,"user_tz":-120,"elapsed":10880,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"db5c845b-f4af-4bc1-9a0b-a10dd889daa1"},"source":["corpus2 = [[(token[0], (token[1] / sum(n for _, n in doc))) for token in doc] for doc in corpus1]\n","\n","print(corpus2[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(0, 0.058823529411764705), (1, 0.29411764705882354), (2, 0.058823529411764705), (3, 0.058823529411764705), (4, 0.058823529411764705), (5, 0.058823529411764705), (6, 0.11764705882352941), (7, 0.058823529411764705), (8, 0.058823529411764705), (9, 0.058823529411764705), (10, 0.058823529411764705), (11, 0.058823529411764705)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BIBz90qOBvsu"},"source":["Here, we perform a one-hot encoding, where we write a one, if the word appears in the document, or zero otherwise. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xD4ewe8U1rUs","executionInfo":{"status":"ok","timestamp":1620024016401,"user_tz":-120,"elapsed":10875,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"971a8303-9fb0-420b-bcd9-19bcdb51084e"},"source":["corpus3 = [[(token[0], 1) for token in doc] for doc in corpus1]\n","\n","print(corpus3[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TVlL0xauB6D0"},"source":["Now, we actually calculate the TF-IDF, taking into account all the documents where each word appears in. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JvZl5v0X2Me-","executionInfo":{"status":"ok","timestamp":1620024018537,"user_tz":-120,"elapsed":13005,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"c82cb137-e02d-4733-d4fa-c03e0093a6c0"},"source":["tfidf = TfidfModel(dictionary=id2word, normalize=True)\n","corpus4 = [tfidf[id2word.doc2bow(doc)] for doc in corpus_gen]\n","\n","print(corpus4[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(0, 0.1438541968916984), (1, 0.8659993628972853), (2, 0.13288860724363213), (3, 0.17566681903543382), (4, 0.1773677410716569), (5, 0.07263746099242874), (6, 0.20301211255383858), (7, 0.1306393577394443), (8, 0.18297928466403593), (9, 0.10636899048674578), (10, 0.1724873903769856), (11, 0.10615603475135126)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F8v2YAQECrqs"},"source":["We want to create a dataframe with the previous calculated frequencies to compare it with the frequencies we got from sklearn. In order to do this, we first get the first document and the words that appear in it to use as columns."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWS_bu4K7VAB","executionInfo":{"status":"ok","timestamp":1620024018537,"user_tz":-120,"elapsed":13000,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"6bdb9b72-02f6-4fab-a251-068eb5d87d4a"},"source":["features = []\n","for key1, value1 in id2word.iteritems():\n","  for key2, value2 in corpus1[0]:\n","    if key1 == key2:\n","      features.append(value1)\n","\n","print(features)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['call', 'car', 'dai', 'engin', 'info', 'know', 'look', 'mail', 'small', 'thank', 'wonder', 'year']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2TXLqLimDb3z"},"source":["Now we can append all the results for the first document into a single dataframe."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"aOfVTbiICGuM","executionInfo":{"status":"ok","timestamp":1620024018538,"user_tz":-120,"elapsed":12997,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"bc90abd1-4107-4360-cc01-5e048852605f"},"source":["doc1 = pd.DataFrame([[pair[1] for pair in corpus1[0]]], columns=features, index=['Abs. Freq.'])\n","doc2 = pd.DataFrame([[pair[1] for pair in corpus2[0]]], columns=features, index=['Rel. Freq'])\n","doc3 = pd.DataFrame([[pair[1] for pair in corpus3[0]]], columns=features, index=['One-Hot'])\n","doc4 = pd.DataFrame([[pair[1] for pair in corpus4[0]]], columns=features, index=['TF-IDF'])\n","\n","corpus_df = doc1.append(doc2).append(doc4).append(doc3)\n","corpus_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>call</th>\n","      <th>car</th>\n","      <th>dai</th>\n","      <th>engin</th>\n","      <th>info</th>\n","      <th>know</th>\n","      <th>look</th>\n","      <th>mail</th>\n","      <th>small</th>\n","      <th>thank</th>\n","      <th>wonder</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Abs. Freq.</th>\n","      <td>1.000000</td>\n","      <td>5.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Rel. Freq</th>\n","      <td>0.058824</td>\n","      <td>0.294118</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.117647</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","    </tr>\n","    <tr>\n","      <th>TF-IDF</th>\n","      <td>0.143854</td>\n","      <td>0.865999</td>\n","      <td>0.132889</td>\n","      <td>0.175667</td>\n","      <td>0.177368</td>\n","      <td>0.072637</td>\n","      <td>0.203012</td>\n","      <td>0.130639</td>\n","      <td>0.182979</td>\n","      <td>0.106369</td>\n","      <td>0.172487</td>\n","      <td>0.106156</td>\n","    </tr>\n","    <tr>\n","      <th>One-Hot</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                call       car       dai  ...     thank    wonder      year\n","Abs. Freq.  1.000000  5.000000  1.000000  ...  1.000000  1.000000  1.000000\n","Rel. Freq   0.058824  0.294118  0.058824  ...  0.058824  0.058824  0.058824\n","TF-IDF      0.143854  0.865999  0.132889  ...  0.106369  0.172487  0.106156\n","One-Hot     1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000\n","\n","[4 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"TNkzpVVjDlR3"},"source":["We print the dataframe that we had generated from the sklearn's frequencies for comparison. We can see that all values are the same, except for the TF-IDF frequencies. This is due to a difference in the formula for the IDF used by sklearn and gensim to calculate the weights, which are the following: \n","\n","Scikit-Learn: $idf(word) = log(\\frac{docs_{total}}{docs_{word}}) + 1$\n","\n","Gensim: $idf(word) = log_2(\\frac{docs_{total}}{docs_{word}})$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"upfiUdNJGZqh","executionInfo":{"status":"ok","timestamp":1620024019078,"user_tz":-120,"elapsed":13532,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"dee12ba9-abad-44c1-c29e-9d4dc72f1f10"},"source":["first_doc_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>call</th>\n","      <th>car</th>\n","      <th>dai</th>\n","      <th>engin</th>\n","      <th>info</th>\n","      <th>know</th>\n","      <th>look</th>\n","      <th>mail</th>\n","      <th>small</th>\n","      <th>thank</th>\n","      <th>wonder</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Abs. Freq.</th>\n","      <td>1.000000</td>\n","      <td>5.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Rel. Freq.</th>\n","      <td>0.058824</td>\n","      <td>0.294118</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.117647</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","      <td>0.058824</td>\n","    </tr>\n","    <tr>\n","      <th>TF-IDF</th>\n","      <td>0.148914</td>\n","      <td>0.850723</td>\n","      <td>0.140981</td>\n","      <td>0.171929</td>\n","      <td>0.173160</td>\n","      <td>0.097393</td>\n","      <td>0.236555</td>\n","      <td>0.139354</td>\n","      <td>0.177220</td>\n","      <td>0.121796</td>\n","      <td>0.169629</td>\n","      <td>0.121642</td>\n","    </tr>\n","    <tr>\n","      <th>One-Hot</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                call       car       dai  ...     thank    wonder      year\n","Abs. Freq.  1.000000  5.000000  1.000000  ...  1.000000  1.000000  1.000000\n","Rel. Freq.  0.058824  0.294118  0.058824  ...  0.058824  0.058824  0.058824\n","TF-IDF      0.148914  0.850723  0.140981  ...  0.121796  0.169629  0.121642\n","One-Hot     1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000\n","\n","[4 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"YaX-94nHIKgx"},"source":["## N-grams"]},{"cell_type":"markdown","metadata":{"id":"x_ZhinPbP10X"},"source":["We now create n-grams by using the CountVectorizer class from Scikit-Learn. We only have to pass the parameter 'ngram_range' to define the size of the wanted combinations. In our case, we are using (2, 2), which means we only want bigrams or combinations of only two words. We also define the parameters 'max_df' and 'min_df' like we have done before, but we can see a difference in this case, where only three features (bigrams) appear in more than 5% of the documents. If we remove the parameter 'min_df', we get over 800,000 features. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5MfphmjIR8D","executionInfo":{"status":"ok","timestamp":1620024023800,"user_tz":-120,"elapsed":18249,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"3ac5c546-170a-4999-c4f2-fcc522a8f414"},"source":["n_gram = CountVectorizer(ngram_range=(2,2), max_df=0.95, min_df=0.05)\n","n_gram_matrix = n_gram.fit_transform(stemmed_data['content'])\n","\n","print(n_gram.get_feature_names())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['articl apr', 'edu write', 'write articl']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zvT1ktvXQ-Ht"},"source":["We transform the matrix we got from the previous step into a dataframe and print the head. We can see that the weights are the absolute frequency of each pair of words. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"6w35CvXcK9Nj","executionInfo":{"status":"ok","timestamp":1620024023801,"user_tz":-120,"elapsed":18245,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"c27f1c3e-220e-4e71-f682-581abf6371c6"},"source":["n_gram_df = pd.DataFrame(n_gram_matrix.todense(), columns=n_gram.get_feature_names())\n","n_gram_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>articl apr</th>\n","      <th>edu write</th>\n","      <th>write articl</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   articl apr  edu write  write articl\n","0           0          0             0\n","1           0          0             0\n","2           0          0             0\n","3           0          1             1\n","4           0          0             0"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"LlBi7VgeRNFZ"},"source":["When we print the maximum for each of the features, we can see that 'article apr' and 'write article' appears a maximum of four times in a single document, while 'edu write' appears two times."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yAJp8eHgLToH","executionInfo":{"status":"ok","timestamp":1620024023801,"user_tz":-120,"elapsed":18240,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"d1b8fc48-d0a2-4ce8-93a3-70ced062b6d9"},"source":["for col in n_gram_df.columns:\n","  print('{}: {}'.format(col, n_gram_df[col].max()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["articl apr: 4\n","edu write: 2\n","write articl: 4\n"],"name":"stdout"}]}]}