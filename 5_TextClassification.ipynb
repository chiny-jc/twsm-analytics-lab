{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"5_TextClassification.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ovRSoHpfno3J"},"source":["# Exercise 5. Text Classification\n","\n","## Text, Web and Social Media Analytics Lab"]},{"cell_type":"markdown","metadata":{"id":"tnqHWZLGn46w"},"source":["In this exercise, we will build multiple classification models for the newsgroup dataset. We will apply the following steps:\n","\n","- Document representation with TF-IDF, Word2Vec and BERT\n","- Naive Bayes Classification Model\n","- Random Forests\n","- Grid Search\n","- BERT for Sequence Classification"]},{"cell_type":"markdown","metadata":{"id":"JFHzmXbdyxvU"},"source":["We first import all the required libraries that we are going to use. "]},{"cell_type":"code","metadata":{"id":"yD6YrRMVniic"},"source":["import pandas as pd\n","import pickle\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.ensemble import RandomForestClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_c-X7jz_fPDx"},"source":["base_directory = '/content/drive/MyDrive/Colab Notebooks/TWSM Analytics Lab/storage/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aZLEIQiIoqcP"},"source":["## Part A: Document Representation with TF-IDF, Word2Vec and BERT"]},{"cell_type":"markdown","metadata":{"id":"lKpZVspVy3Rk"},"source":["We load the three datasets that we had previously preprocessed. "]},{"cell_type":"code","metadata":{"id":"2skaMkDRoc6-"},"source":["data_stem = pickle.load(open(base_directory + 'stemmed_data.p', 'rb'))\n","data_w2v = pickle.load(open(base_directory + 'WordtoVecModel.pkl', 'rb'))\n","data_bert = pickle.load(open(base_directory + 'BertModel.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSes7ZMIzBAc"},"source":["We show the head of the stemmed dataset to check its contents."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"b-xJ7vYepP_e","executionInfo":{"status":"ok","timestamp":1621236001430,"user_tz":-120,"elapsed":922,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"28045ef9-1089-494e-9153-6fa5a9c5d88b"},"source":["data_stem.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>target</th>\n","      <th>target_names</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>car wonder enlighten car saw dai door sport ca...</td>\n","      <td>7</td>\n","      <td>rec.autos</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>clock poll final clock report acceler clock up...</td>\n","      <td>4</td>\n","      <td>comp.sys.mac.hardware</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>question folk mac plu final gave ghost weekend...</td>\n","      <td>4</td>\n","      <td>comp.sys.mac.hardware</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>weitek robert kyanko rob rjck uucp wrote abrax...</td>\n","      <td>1</td>\n","      <td>comp.graphics</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>shuttl launch question articl cowcb world std ...</td>\n","      <td>14</td>\n","      <td>sci.space</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             content  ...           target_names\n","0  car wonder enlighten car saw dai door sport ca...  ...              rec.autos\n","1  clock poll final clock report acceler clock up...  ...  comp.sys.mac.hardware\n","2  question folk mac plu final gave ghost weekend...  ...  comp.sys.mac.hardware\n","3  weitek robert kyanko rob rjck uucp wrote abrax...  ...          comp.graphics\n","4  shuttl launch question articl cowcb world std ...  ...              sci.space\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"qHyMPPf4zPR2"},"source":["Since the Word2Vector model only includes the document embeddings, we add the 'target' and 'target_names' columns from the stemmed dataset, because we will be needing these two columns for the classification task."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ti_oBoUOpYLm","executionInfo":{"status":"ok","timestamp":1621236001643,"user_tz":-120,"elapsed":1129,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"dcd25800-bdb8-4dc1-9ee0-440918091944"},"source":["data_w2v = data_w2v.join(data_stem[['target','target_names']], on=data_w2v.index, how='left')\n","\n","print(data_w2v.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["          0         1         2  ...        99  target           target_names\n","0  0.145009  0.074307  0.062820  ... -0.051636       7              rec.autos\n","1 -0.328259 -0.023729  0.118994  ... -0.025866       4  comp.sys.mac.hardware\n","2 -0.236099  0.099917  0.135177  ... -0.233269       4  comp.sys.mac.hardware\n","3 -0.143364 -0.172010  0.220363  ... -0.076263       1          comp.graphics\n","4 -0.043743  0.041052 -0.155883  ... -0.262626      14              sci.space\n","\n","[5 rows x 102 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k5Iz7xv6zlis"},"source":["The same as we did before, we add the columns 'target' and 'target_names' to the BERT data, since it only included the embeddings as well. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2u_btFBDp_5U","executionInfo":{"status":"ok","timestamp":1621236001644,"user_tz":-120,"elapsed":1124,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"1b950fb7-d659-4b25-9f78-bcf34b4bf1f3"},"source":["data_bert = data_bert.join(data_stem[['target', 'target_names']], on=data_bert.index, how='left')\n","\n","print(data_bert.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["          0         1         2  ...       767  target           target_names\n","0 -0.127447 -0.267551 -0.952460  ... -0.155924       7              rec.autos\n","1 -0.128594 -0.297495 -0.960266  ... -0.225011       4  comp.sys.mac.hardware\n","2 -0.364291 -0.418737 -0.972758  ... -0.155387       4  comp.sys.mac.hardware\n","3 -0.208075 -0.404881 -0.984882  ... -0.209980       1          comp.graphics\n","4 -0.134173 -0.371447 -0.981081  ... -0.286874      14              sci.space\n","\n","[5 rows x 770 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Al7yBcELzwz0"},"source":["We decide to use only a specific subset of topics, so we remove all the rows from all the datasets that do not belong to these topics."]},{"cell_type":"code","metadata":{"id":"HrP-waKbq5Pw"},"source":["topics = ['soc.religion.christian', 'rec.sport.hockey', 'talk.politics.mideast', 'rec.motorcycles']\n","\n","data_stem = data_stem[data_stem['target_names'].isin(topics)]\n","data_w2v = data_w2v[data_w2v['target_names'].isin(topics)]\n","data_bert = data_bert[data_bert['target_names'].isin(topics)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nr4B6E-4z_ys"},"source":["Since we saw before that the stemmed dataset still included the documents, we decide to calculate the TF-IDF frequency and create a new dataframe with the 'target' and 'target_names' columns additionally. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PA__5aE1sQ8o","executionInfo":{"status":"ok","timestamp":1621236002069,"user_tz":-120,"elapsed":1538,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"d58d4434-28fb-48d6-e906-02a69ee6d7ce"},"source":["tfidf_frequency = TfidfVectorizer(max_df=0.7, min_df=0.1).fit_transform(data_stem['content'])\n","data_stem2 = pd.DataFrame(tfidf_frequency.toarray()).join(data_stem[['target', 'target_names']], on=data_stem.index, how='left')\n","\n","print(data_stem2.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["          0         1    2  ...        86  target            target_names\n","0  0.000000  0.000000  0.0  ...  0.000000       8         rec.motorcycles\n","1  0.169876  0.132560  0.0  ...  0.000000      10        rec.sport.hockey\n","2  0.097571  0.076138  0.0  ...  0.000000      15  soc.religion.christian\n","3  0.181629  0.212597  0.0  ...  0.000000      17   talk.politics.mideast\n","4  0.000000  0.136480  0.0  ...  0.195978      10        rec.sport.hockey\n","\n","[5 rows x 89 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QPrSqNOcJPMq"},"source":["Before going into training the models, we define a function which splits the data into the independent variables, which are all columns except for the last two, and the target variable, which is the 'target' column. We also define a test size of 20% the size of the whole dataset and a random state so the results can be reproduced in different circumstances."]},{"cell_type":"code","metadata":{"id":"eWjZFBn1I-dk"},"source":["def text_train(df):\n","  return train_test_split(df.iloc[:, :-2], df.target, test_size=0.20, random_state=12)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-EBVhZVWu-9g"},"source":["## Part B: Naive Bayes Classification Model"]},{"cell_type":"markdown","metadata":{"id":"lkORTgNuR1G4"},"source":["First we split our stemmed data into training and testing set for the independent and target variables. We then define a Naive Bayes model and fit the data to it. We finally predict on our test data. "]},{"cell_type":"code","metadata":{"id":"3ZK72j5mKEHL"},"source":["docs_train_s, docs_test_s, y_train_s, y_test_s = text_train(data_stem2)\n","\n","clf = MultinomialNB()\n","clf.fit(docs_train_s, y_train_s)\n","\n","y_pred_s = clf.predict(docs_test_s)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4BTD_dRISLvb"},"source":["We print the score, the accuracy and the classification report, where we actually see that the model did not perform bad at all. According to the f1-score, we can see that the model had some more troubles for class 17, which is 'talk.politics.mideast', but overall it performed quite well.  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBEijkaGKQG3","executionInfo":{"status":"ok","timestamp":1621236002071,"user_tz":-120,"elapsed":1527,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"199eb5ae-23e3-43ba-a0ea-36f0a23697a8"},"source":["print('Training Score: {}'.format(round(clf.score(docs_train_s, y_train_s), 4)))\n","print('Accuracy: {}'.format(round(accuracy_score(y_pred_s, y_test_s), 4)))\n","print('Classification Report:\\n{}'.format(classification_report(y_pred_s, y_test_s)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Score: 0.8528\n","Accuracy: 0.833\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           8       0.87      0.83      0.85       140\n","          10       0.86      0.87      0.87       117\n","          15       0.87      0.81      0.84       113\n","          17       0.73      0.82      0.77       103\n","\n","    accuracy                           0.83       473\n","   macro avg       0.83      0.83      0.83       473\n","weighted avg       0.84      0.83      0.83       473\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wRiIepIfLVNX"},"source":["## Part C: Random Forests"]},{"cell_type":"markdown","metadata":{"id":"WXKP8eWJS6_7"},"source":["We now define a Random Forest classifier and fit the same data we used in the previous model. We then predict on our test data and print the classification report, where we see that this model actually performed even better than the Naive Bayes. There is a slight increase in the average accuracy, but we can see that the f1-scores are all quite high. One thing to notice is that the training score is very high, which means that the model is overfitting to the training data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2zIp2RPMK0Bt","executionInfo":{"status":"ok","timestamp":1621236002732,"user_tz":-120,"elapsed":2182,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"3bdaebc7-394f-438e-b8d7-f9dcce25cb6e"},"source":["clf2 = RandomForestClassifier(random_state=42)\n","clf2.fit(docs_train_s, y_train_s)\n","\n","y_pred_s = clf2.predict(docs_test_s)\n","\n","print('Training Score: {}'.format(round(clf2.score(docs_train_s, y_train_s), 4)))\n","print(classification_report(y_pred_s, y_test_s))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Score: 0.9936\n","              precision    recall  f1-score   support\n","\n","           8       0.86      0.92      0.89       125\n","          10       0.89      0.82      0.85       128\n","          15       0.85      0.84      0.85       107\n","          17       0.86      0.88      0.87       113\n","\n","    accuracy                           0.86       473\n","   macro avg       0.86      0.86      0.86       473\n","weighted avg       0.87      0.86      0.86       473\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dwxvwnj-TsQb"},"source":["Here we also define a new Random Forest classifier and train it, but this time with another dataset. So first, we split our Word2Vector data into training and testing set, which we use to train and predict. We then print the classification report, where we can see that the model is performing poorly with an average accuracy of 0.25. We can also see that the training score is also very high, which means that the classifier overfitted and its poor performance on the test set might be happening because of the increased number of features and value ranges of the Word2Vec embeddings."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1I-Iva8wMWP0","executionInfo":{"status":"ok","timestamp":1621236004267,"user_tz":-120,"elapsed":3711,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"65944ffc-c598-4bab-f1a2-05e19f93fe6d"},"source":["docs_train_w, docs_test_w, y_train_w, y_test_w = text_train(data_w2v)\n","\n","clf3 = RandomForestClassifier(random_state=42)\n","clf3.fit(docs_train_w, y_train_w)\n","\n","y_pred_w = clf3.predict(docs_test_w)\n","\n","print('Training Score: {}'.format(round(clf3.score(docs_train_w, y_train_w), 4)))\n","print(classification_report(y_pred_w, y_test_w))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Score: 0.9984\n","              precision    recall  f1-score   support\n","\n","           8       0.26      0.32      0.29       115\n","          10       0.27      0.21      0.24       140\n","          15       0.27      0.21      0.23       135\n","          17       0.19      0.27      0.23        82\n","\n","    accuracy                           0.25       472\n","   macro avg       0.25      0.25      0.25       472\n","weighted avg       0.25      0.25      0.25       472\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qb9oCVcPUOew"},"source":["Like we did before, here we also define a new Random Forst classifier and train it using another dataset. This time we split the BERT dataset into the respective sets, train our model and predict on our test data. We print the classification report, where we see that the model performed better than the previous one, but still worse than the first random forest classifier. The training score is of 100%, which means this model also overfitted; however, the test score is better than the previous one, which might show how powerful the BERT embeddings are. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a5sA_ca3M1Qv","executionInfo":{"status":"ok","timestamp":1621236007277,"user_tz":-120,"elapsed":6715,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"b8640363-8abf-406b-a87b-77894a074013"},"source":["docs_train_b, docs_test_b, y_train_b, y_test_b = text_train(data_bert)\n","\n","clf4 = RandomForestClassifier(random_state=42)\n","clf4.fit(docs_train_b, y_train_b)\n","\n","y_pred_b = clf4.predict(docs_test_b)\n","\n","print('Training Score: {}'.format(round(clf4.score(docs_train_b, y_train_b), 4)))\n","print(classification_report(y_pred_b, y_test_b))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Score: 1.0\n","              precision    recall  f1-score   support\n","\n","           8       0.70      0.69      0.70       136\n","          10       0.69      0.64      0.66       126\n","          15       0.75      0.65      0.70       124\n","          17       0.55      0.72      0.62        87\n","\n","    accuracy                           0.67       473\n","   macro avg       0.67      0.68      0.67       473\n","weighted avg       0.68      0.67      0.67       473\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k2elEan3NgV3"},"source":["## Part D: Grid Search"]},{"cell_type":"markdown","metadata":{"id":"2Var6RpaU54F"},"source":["All machine learning models have parameters that can be tuned in order to improve the model's performance. One way to do this is by defining a set of parameters with which several models will be trained with and a comparison of the model's performance is made to select the best-performing parameters.\n","\n","To do this, we first define a parameter grid with a couple of values for the 'min_samples_leaf' and 'n_estimators' parameters. We then initialize a new Random Forest Classifier, as well as a GridSearchCV object, to which we pass the estimator and the parameter grid in order to train the models."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NoArHYHnNSti","executionInfo":{"status":"ok","timestamp":1621236007770,"user_tz":-120,"elapsed":7204,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"b819232c-6e82-4c4c-93ce-32812b6028b5"},"source":["param_grid = {'min_samples_leaf': [5, 10], 'n_estimators': [3, 5]}\n","\n","rf = RandomForestClassifier(random_state=42)\n","grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=10)\n","grid_search.fit(docs_train_s, y_train_s)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=10, error_score=nan,\n","             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n","                                              class_weight=None,\n","                                              criterion='gini', max_depth=None,\n","                                              max_features='auto',\n","                                              max_leaf_nodes=None,\n","                                              max_samples=None,\n","                                              min_impurity_decrease=0.0,\n","                                              min_impurity_split=None,\n","                                              min_samples_leaf=1,\n","                                              min_samples_split=2,\n","                                              min_weight_fraction_leaf=0.0,\n","                                              n_estimators=100, n_jobs=None,\n","                                              oob_score=False, random_state=42,\n","                                              verbose=0, warm_start=False),\n","             iid='deprecated', n_jobs=None,\n","             param_grid={'min_samples_leaf': [5, 10], 'n_estimators': [3, 5]},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring=None, verbose=0)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"zWoUrnFQXaaN"},"source":["We can check what value combination of the parameters performed the best."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCNOCUS-0Ytc","executionInfo":{"status":"ok","timestamp":1621236007771,"user_tz":-120,"elapsed":7202,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"a041c47c-78df-41d8-c01b-f76364d0b472"},"source":["best_params = grid_search.best_params_\n","\n","print('Best Parameters:\\n{}'.format(best_params))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best Parameters:\n","{'min_samples_leaf': 5, 'n_estimators': 5}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YwhEDOGCXfaj"},"source":["We get the best model/estimator, which is the model that was trained with the previous parameters, and then predict on our test data. We then print the classification report and see that the model performed well, but not as well as previous models. However, in this model we can see that the training and testing score is closer together, which might mean that overfitting is reduced with these parameters."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7DOHqUKT0n8I","executionInfo":{"status":"ok","timestamp":1621236008173,"user_tz":-120,"elapsed":7601,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"0328b944-a7e2-474e-f6f6-239369a61196"},"source":["best_model = grid_search.best_estimator_\n","y_pred_bm = best_model.predict(docs_test_s)\n","\n","print('Training Score: {}'.format(round(best_model.score(docs_train_s, y_train_s), 4)))\n","print(classification_report(y_pred_bm, y_test_s))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Score: 0.8893\n","              precision    recall  f1-score   support\n","\n","           8       0.83      0.85      0.84       131\n","          10       0.88      0.82      0.85       127\n","          15       0.78      0.82      0.80       101\n","          17       0.79      0.80      0.79       114\n","\n","    accuracy                           0.82       473\n","   macro avg       0.82      0.82      0.82       473\n","weighted avg       0.82      0.82      0.82       473\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y195Mm6AX6GR"},"source":["Here we print the classification report of the first Random Forest Classifier that we trained for comparison, which performed slightly better than the previous model, but overfitted on the training set."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2hJAJDP1ec8","executionInfo":{"status":"ok","timestamp":1621236008173,"user_tz":-120,"elapsed":7598,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"9fe73f8a-3dee-4dec-e1af-bd4238263ef6"},"source":["print('First Random Forest Results')\n","print('Training Score: {}'.format(round(clf2.score(docs_train_s, y_train_s), 4)))\n","print(classification_report(y_pred_s, y_test_s))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["First Random Forest Results\n","Training Score: 0.9936\n","              precision    recall  f1-score   support\n","\n","           8       0.86      0.92      0.89       125\n","          10       0.89      0.82      0.85       128\n","          15       0.85      0.84      0.85       107\n","          17       0.86      0.88      0.87       113\n","\n","    accuracy                           0.86       473\n","   macro avg       0.86      0.86      0.86       473\n","weighted avg       0.87      0.86      0.86       473\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9hhmm_702XI8"},"source":["## Part E: BERT Model"]},{"cell_type":"markdown","metadata":{"id":"3XFpNHZSYKV2"},"source":["In order to train a BERT model, we have to make sure to have the transformers package, which we install here. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WYj33kDYfsEW","executionInfo":{"status":"ok","timestamp":1620774521978,"user_tz":-120,"elapsed":10003,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"5b32bbe4-0e21-4f4b-ec70-ea2499d98bba"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7u6orf3OYPvJ"},"source":["We import some additional libraries that we will be using to train our next model."]},{"cell_type":"code","metadata":{"id":"jbtE8Ofa3J1Y"},"source":["import numpy as np\n","import random\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n","from torch.nn.functional import softmax\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n","from keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I0uAXitMYYVg"},"source":["We check if we have an available GPU, which we then tell PyTorch to use. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29oObN-W5eyO","executionInfo":{"status":"ok","timestamp":1620774523898,"user_tz":-120,"elapsed":11916,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"778e7c11-3efd-4bea-bf06-4d2ec34f7feb"},"source":["if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    print('There are %d GPU(s) available.' %torch.cuda.device_count())\n","    print('We will use the GPU: ', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device('cpu')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU:  Tesla T4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RSLvcCcTYqXA"},"source":["We will first do some preprocessing again, which most was done in the previous exercise, but we need to do some changes to the data for our BERT model. \n","\n","We load our lemmatized data from the pickle file that we had saved before. We then filter by the four topics that we had chosen previously and change the target values respectively, so they range from 0 to 3 for a four class classification task. We then print the head of our dataframe to see that the filtering and the changes we done correctly."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"Ire3ini83eNU","executionInfo":{"status":"ok","timestamp":1620774523898,"user_tz":-120,"elapsed":11910,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"39e497dd-8f2f-4526-f3c3-7957ed33cdf2"},"source":["lemmatized_data = pickle.load(open(base_directory + 'lemmatized_data.p', 'rb'))\n","\n","topics = {'soc.religion.christian':0, 'rec.sport.hockey':1, 'talk.politics.mideast':2, 'rec.motorcycles':3}\n","lemmatized_data = lemmatized_data[lemmatized_data['target_names'].isin(topics.keys())]\n","lemmatized_data['target'] = lemmatized_data['target_names'].apply(lambda name: topics[name])\n","\n","lemmatized_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>target</th>\n","      <th>target_names</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>10</th>\n","      <td>recommendation duc worth ducati gts line ducat...</td>\n","      <td>3</td>\n","      <td>rec.motorcycles</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>nhl team captain article apr samba oit unc edu...</td>\n","      <td>1</td>\n","      <td>rec.sport.hockey</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>pantheism environmentalism article apr athos r...</td>\n","      <td>0</td>\n","      <td>soc.religion.christian</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>israeli expansion lust article spam math adela...</td>\n","      <td>2</td>\n","      <td>talk.politics.mideast</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>goalie masks article netnews upenn edu kkeller...</td>\n","      <td>1</td>\n","      <td>rec.sport.hockey</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              content  ...            target_names\n","10  recommendation duc worth ducati gts line ducat...  ...         rec.motorcycles\n","21  nhl team captain article apr samba oit unc edu...  ...        rec.sport.hockey\n","28  pantheism environmentalism article apr athos r...  ...  soc.religion.christian\n","33  israeli expansion lust article spam math adela...  ...   talk.politics.mideast\n","35  goalie masks article netnews upenn edu kkeller...  ...        rec.sport.hockey\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"iDyj0wEAZhEC"},"source":["We add the '[CLS]' and '[SEP]' markers at the beginning and at the end of each sentences respectively, since BERT needs them. We then tokenize each of the sentences and print the first one to check how it looks."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xiA-VzI351Vg","executionInfo":{"status":"ok","timestamp":1620774532516,"user_tz":-120,"elapsed":20523,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"90255c78-6665-4bc4-e35f-cf7cd2d52f42"},"source":["sentences = ['[CLS] ' + query + ' [SEP]' for query in lemmatized_data['content']]\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","tokenized_texts = [tokenizer.tokenize(sentence) for sentence in sentences]\n","\n","print(tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['[CLS]', 'recommendation', 'duc', 'worth', 'duc', '##ati', 'gt', '##s', 'line', 'duc', '##ati', 'gt', '##s', 'model', 'clock', 'run', 'paint', 'bronze', 'brown', 'orange', 'fade', 'leak', 'bit', 'oil', 'pop', 'hard', 'acc', '##el', 'shop', 'fix', 'trans', 'oil', 'leak', 'sell', 'bike', 'owner', 'want', 'think', 'like', 'opinion', 'email', 'thank', 'nice', 'stable', 'mate', 'bee', '##mer', 'ja', '##p', 'bike', 'axis', 'motors', 'tuba', 'irwin', 'hon', '##k', 'com', '##put', '##rac', 'richardson', 'irwin', 'cm', '##pt', '##rc', 'lone', '##star', 'org', 'dod', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WxS9fjyiaD22"},"source":["We now perform some padding and truncating to each of our documents. We define the 'maxlen' parameter to 512 since that is the size BERT works with, the padding value as '[PAD]', and define that the padding and truncating should be performed 'post', which means after the actual tokens. We then print the first document after the transformations to see how it looks like, but we just show the first 77 characters, since the rest will only include the '[PAD]' marker."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-rKgnWl6mkj","executionInfo":{"status":"ok","timestamp":1620774532516,"user_tz":-120,"elapsed":20518,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"48a56173-3b52-41e6-d239-b4a20a6b1bd2"},"source":["sentences_padded = pad_sequences(tokenized_texts, dtype=object, maxlen=512, value='[PAD]', truncating='post', padding='post')\n","\n","print(sentences_padded[0][:77])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['[CLS]' 'recommendation' 'duc' 'worth' 'duc' '##ati' 'gt' '##s' 'line'\n"," 'duc' '##ati' 'gt' '##s' 'model' 'clock' 'run' 'paint' 'bronze' 'brown'\n"," 'orange' 'fade' 'leak' 'bit' 'oil' 'pop' 'hard' 'acc' '##el' 'shop' 'fix'\n"," 'trans' 'oil' 'leak' 'sell' 'bike' 'owner' 'want' 'think' 'like'\n"," 'opinion' 'email' 'thank' 'nice' 'stable' 'mate' 'bee' '##mer' 'ja' '##p'\n"," 'bike' 'axis' 'motors' 'tuba' 'irwin' 'hon' '##k' 'com' '##put' '##rac'\n"," 'richardson' 'irwin' 'cm' '##pt' '##rc' 'lone' '##star' 'org' 'dod'\n"," '[SEP]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]' '[PAD]']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W3L5w9SdbLiA"},"source":["Now we convert each of the tokens to its id. We print the first 77 characters of the first document, where we can identify the values for certain markers. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"481_jm_J7P7s","executionInfo":{"status":"ok","timestamp":1620774533742,"user_tz":-120,"elapsed":21737,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"19ecac66-a96b-4c53-bfdd-24364bf15469"},"source":["sentences_converted = [tokenizer.convert_tokens_to_ids(token) for token in sentences_padded]\n","\n","print(sentences_converted[0][:77])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[101, 12832, 26363, 4276, 26363, 10450, 14181, 2015, 2240, 26363, 10450, 14181, 2015, 2944, 5119, 2448, 6773, 4421, 2829, 4589, 12985, 17271, 2978, 3514, 3769, 2524, 16222, 2884, 4497, 8081, 9099, 3514, 17271, 5271, 7997, 3954, 2215, 2228, 2066, 5448, 10373, 4067, 3835, 6540, 6775, 10506, 5017, 14855, 2361, 7997, 8123, 9693, 29242, 17514, 10189, 2243, 4012, 18780, 22648, 9482, 17514, 4642, 13876, 11890, 10459, 14117, 8917, 26489, 102, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UdxO-XRXbcO7"},"source":["We also create a mask for each document, where we write a one if the token id is higher than zero, or zero if it's lower. In this case, all '[PAD]' markers will become zeros and every other token will be a one. We do the same print as before to check if this is really happening."]},{"cell_type":"code","metadata":{"id":"wx_rQ6bJ7pxC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620774534084,"user_tz":-120,"elapsed":22075,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"fe6e3f70-3f70-4da8-b918-c68390148d4b"},"source":["masks = []\n","\n","for seq in sentences_converted:\n","  seq_mask = [int(i>0) for i in seq]\n","  masks.append(seq_mask)\n","    \n","print(masks[0][:77])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jBCre03qb1Lj"},"source":["Now that our data is preprocessed and ready, we must split it into training and testing sets. We split the ids, the masks and the labels the same way we splitted the datasets for the previous models. This way we will be able to compare accurately. We then print the train and test lengths of the data to make sure all the data was split correctly."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ADxxoDmKdfg","executionInfo":{"status":"ok","timestamp":1620774534085,"user_tz":-120,"elapsed":22069,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"8b2b5ab8-d2be-4672-fcf2-e855de73e211"},"source":["inputs_train, inputs_test, labels_train, labels_test = train_test_split(sentences_converted, lemmatized_data['target'], \n","                                                                        test_size=0.2, random_state=12)\n","masks_train, masks_test = train_test_split(masks, test_size=0.2, random_state=12)\n","\n","print('Train-Test Lengths\\nInputs: {} - {}\\nMasks: {} - {}\\nLabels: {} - {}'.format(len(inputs_train), \n","            len(inputs_test), len(masks_train), len(masks_test), len(labels_train), len(labels_test)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train-Test Lengths\n","Inputs: 1888 - 473\n","Masks: 1888 - 473\n","Labels: 1888 - 473\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7lNEhs4ycdzb"},"source":["We now convert all of our dataset variables into torch tensors. "]},{"cell_type":"code","metadata":{"id":"gfAsIGZc7rh1"},"source":["inputs_train = torch.LongTensor(inputs_train)\n","inputs_test = torch.LongTensor(inputs_test)\n","\n","labels_train = torch.tensor(labels_train.values)\n","labels_test = torch.tensor(labels_test.values)\n","\n","masks_train = torch.LongTensor(masks_train)\n","masks_test = torch.LongTensor(masks_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XHB2AbZDclzd"},"source":["We define the batch size we want our model to use for the training, as well as create some more variables. We first create a TensorDataset with all of our training set variables, we also create a SequentialSampler with our train data and a DataLoader for our training session."]},{"cell_type":"code","metadata":{"id":"GG02dKXg_AxZ"},"source":["batch_size = 8\n","\n","train_data = TensorDataset(inputs_train, masks_train, labels_train)\n","train_sampler = SequentialSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mqvXclMSdIna"},"source":["Before we start the training, we have to change the BERT model configuration, since by default it's configured for a binary classification. In our case, we have four classes, so that's the number of labels we have to define. We then create a new BertForSequenceClassification model with our configurations and send it to our GPU. We also define a new AdamW optimizer for our model."]},{"cell_type":"code","metadata":{"id":"zVPZvl5H8PcD","scrolled":true},"source":["config = BertConfig.from_pretrained('bert-base-uncased')\n","config.num_labels = 4\n","\n","model = BertForSequenceClassification(config)\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters(), lr=1e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OfESVRPXdw6Q"},"source":["We first have to set the state of the model to train, before we actually do it. We then decide to train the model for five epochs and then loop through each batch of data from the data loader. Inside each loop, we get the ids, masks and labels that were inside the data loader and send it to our GPU. We then set the gradients to zero and make the forward pass on the model. We get the outputs, from which we can get the loss, then we perform the backward propagation and update the parameters on the optimizer.  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w7i5sYpEQsFq","executionInfo":{"status":"ok","timestamp":1620775486620,"user_tz":-120,"elapsed":974592,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"a26f4050-d7ba-4854-8fd1-8d049cccd116"},"source":["model.train()\n","\n","for epoch in range(5):\n","\n","  for batch in train_dataloader:\n","      \n","      b_input_ids = batch[0].to(device)\n","      b_input_masks = batch[1].to(device)\n","      b_labels = batch[2].to(device)\n","      \n","      model.zero_grad()\n","      \n","      outputs = model(b_input_ids, attention_mask=b_input_masks, labels=b_labels)\n","      loss = outputs.loss\n","      loss.backward()\n","      optimizer.step()\n","    \n","print('Training Done...')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Done...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bQBy34JNfGF2"},"source":["Similarly than with the train data, we also create a TensorDataset with our test dataset variables, as well as a SequentialSampler and a DataLoader. "]},{"cell_type":"code","metadata":{"id":"2Xu_frHgYDgg"},"source":["test_data = TensorDataset(inputs_test, masks_test, labels_test)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-9OHKuugfdfB"},"source":["We now change the state of the model to evaluation. We create an empty list for our batch predictions and then tell torch to not update the gradients, since we will be testing and no longer training. For each batch in the test data loader, we get the ids and the masks only. We then pass these to our model for a forward pass and get the outputs. We pass the logits from the outputs to a softmax function to get the probabilities for each class. We then get the class with the higher probability, which will be our predicted class, detach it from the GPU and append it to our batch predictions."]},{"cell_type":"code","metadata":{"id":"HwLqD7QQhJeX"},"source":["model.eval()\n","\n","batch_predictions = []\n","\n","with torch.no_grad():\n","  \n","  for batch in test_dataloader:\n","\n","    b_input_ids = batch[0].to(device)\n","    b_input_masks = batch[1].to(device) \n","\n","    outputs = model(b_input_ids, attention_mask=b_input_masks)\n","\n","    probs = softmax(outputs.logits, dim=1)\n","    predicted = torch.max(probs, 1).indices\n","    predicted_detached = predicted.detach().cpu().numpy()\n","    batch_predictions.append(predicted_detached)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ROKCB7xgX1j"},"source":["We then loop through each batch and append each class value to a flat list. We then print the length to make sure everything was executed correctly and indeed it matches with the test size. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPvoLeHal-UZ","executionInfo":{"status":"ok","timestamp":1620775503897,"user_tz":-120,"elapsed":991859,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"7613ee56-736c-4540-c68c-96b7fc86bdb1"},"source":["predictions = []\n","\n","for batch in batch_predictions:\n","  for value in batch:\n","    predictions.append(value)\n","\n","print(len(predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["473\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hnIIKDj4glxC"},"source":["Finally, we now print the classification report, where we can see that the BERT model outperformed any other model that we tried before and by a really surprising difference with an 98% accuracy."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pFvysnVzn9fD","executionInfo":{"status":"ok","timestamp":1620775503897,"user_tz":-120,"elapsed":991854,"user":{"displayName":"Joshua Campos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgNBYf7hbBqn4J1KUTIQ-AfTLBSfnvPEaVkswsDbg=s64","userId":"15520650830739661074"}},"outputId":"a6db4073-d753-42c5-97f4-8b067d983c48"},"source":["print(classification_report(labels_test.numpy(), np.array(predictions)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.99      0.98       106\n","           1       1.00      0.95      0.97       118\n","           2       1.00      0.97      0.99       115\n","           3       0.95      0.99      0.97       134\n","\n","    accuracy                           0.98       473\n","   macro avg       0.98      0.98      0.98       473\n","weighted avg       0.98      0.98      0.98       473\n","\n"],"name":"stdout"}]}]}